### Purpose

This code covers model fitting for the parks VIC hmsc project. Part 2 (these codes) look at defining and fitting models then looking at MCMC convergence and the associated model fit.

### 0) Setup - identify working directory and set seed

```{r setup} 

knitr::opts_knit$set(root.dir = "Z:/GIS-Res/Projects/ParksVic_Hmsc/Modelling/")
set.seed(1995)

```

### 1) Package Import

```{r}
library(Hmsc)
library(corrplot)
library(dplyr)
library(ape)
library(stringr)
```

### 3) Directories

```{r}
localDir = "Z:/GIS-Res/Projects/ParksVic_Hmsc/Modelling/"
data.directory = file.path(localDir, "ResponseCSVs")
unfitted.model.directory = file.path(localDir, "UnfittedModels")
fitted.model.directory = file.path(localDir, "FittedModels")
```

### 4) Check intercorrelation

First off, we need to see how many of our environmental predictors are intercorrelated. We have loads of measures of similar variables (e.g. 5 temp measures, 5 wave measures) so there is likely to be a bit to cut here. Like other models, HMSC doesnt really want you to add loads of predictors that each represent the same ecological phenomena.

```{r}
#Import fish and invert data
fishData = read.csv(file.path(data.directory, "fishDataFinal.csv"), stringsAsFactors = TRUE)
invertData = read.csv(file.path(data.directory, "invertDataFinal.csv"), stringsAsFactors = TRUE)

#Check intercor of all environmental predictors. Looking for Pearsons >0.7.
cor1 = fishData[,13:29] %>% cor(method = "pearson")
corrplot(cor1, method = "number", type = "lower", diag = FALSE)

#adjSD doesnt play well with vrm or qslope. The latter two dont play together either. All wave measures are >0.7. Temp doesn't look much better.
#Try again with just monthly measure and SD for wave and temp. And drop Qslope and vrm. 
cor1 = fishData[,c(13:18, 20:21, 25, 26,29)] %>% cor(method = "pearson")
corrplot(cor1, method = "number", type = "lower", diag = FALSE)

#Terrains are better but the waves still trip us up. Unsurprisingly monthly mean is correlated with the monthly SD. Dropping the latter.
cor1 = fishData[,c(13:18, 20:21, 25, 29)] %>% cor(method = "pearson")
corrplot(cor1, method = "number", type = "lower", diag = FALSE)

#Lets run with that.
fishData = fishData[,c(1:12, 13:18, 20:21, 25, 29, 30:279)]

#Repeat for inverts.....
#Check intercor of all environmental predictors. Looking for Pearsons <0.7.
cor1 = invertData[,13:29] %>% cor(method = "pearson")
corrplot(cor1, method = "number", type = "lower", diag = FALSE)

cor1 = invertData[,c(13:18, 20:21, 25, 26,29)] %>% cor(method = "pearson")
corrplot(cor1, method = "number", type = "lower", diag = FALSE)

#Terrains are better but the waves still trip us up. Repeat for just seasonal mean for temp and seasonal 95th for waves.
cor1 = invertData[,c(13:18, 20:21, 25, 29)] %>% cor(method = "pearson")
corrplot(cor1, method = "number", type = "lower", diag = FALSE)

#Lets run with that. Unsurprisingly identical process as I think the RSL survey data take place at the same locales. Marginally different col numbers mind. Different N species found.
invertData = invertData[,c(1:12, 13:18, 20:21, 25, 29, 30:285)]

```

### 5) Prep matrices

Check data structure and amend as needed.

```{r}
str(fishData)
fishData$year = as.factor(fishData$year)

str(invertData)
invertData$year = as.factor(invertData$year)

```

Extract X matrix - environmental variables. Time included as T since 2002 (when the MPAs were established). Protection status included as a factor. As is bioregion.

```{r}
fishX = fishData[,c(4,7,8,13:22)]

invertX = invertData[,c(4,7,8,13:22)]

```

Extract Y matrix - occurences and abundances.

```{r}
fishY = (fishData[,23:272])
invertY = (invertData[,23:278])

#Check for absent or ubiquitous species. Fact it returns a value tells us we have no NAs in our rows too
range(colMeans(fishY>0))
min(colSums(fishY>0)) #Some fish weren't found in any samples

range(colMeans(invertY>0))
min(colSums(invertY>0)) #Some inverts weren't found in any samples

#Check the number of rare taxa (those present in less than 5% of samples)
rareFish <- which(colSums(fishY>0)<48) #48 is ~5% of samples
length(rareFish) #192 rare fish

rareInvert <- which(colSums(invertY>0)<49) #49 is ~5% of samples
length(rareInvert) #210 rare inverts

#Check the number of overabundant taxa (those present in greater than 95% of samples)
ovabunFish <- which(colSums(fishY>0)>920) #920 is ~95% of samples
length(ovabunFish) #0 overly abundant fish

ovabunInvert <- which(colSums(invertY>0)>931) #931 is ~95% of samples
length(ovabunInvert) #0 overly abundant inverts

#Drop rare or overabundant species. We dont drop overabundant ones as it only selected one sp and we care about tetricus!
fishY <- fishY[ , -c(rareFish)]
invertY <- invertY[ , -c(rareInvert)]

#Dropping the rare species leaves 58 fish and 46 inverts.

#Assess abundance histogram, prevalence, and log abundance conditional on presence
hist(colMeans(fishY), nclass = 35, main = "Fish - Mean Abun per sample unit") #Most fish are in super low densities (<6 abundance per sample) per sample unit but 8-10 (25% of all fish left in the data) have higher abundance spanning up 35 individuals per sample on average. Huge 0 spike here.
hist(colMeans(fishY>0), main = "Fish - Prevalence") #Where a given fish species is present most have <0.3 prevalence. 9 or so appear to have >0.4 prevalence.
hist(log(fishY[fishY>0]), main = "Fish - log abundance conditional on presence") #Across all fish, even when logged, the abundance is still skewed towards small values. Reduced 0 spike compared to untransformed data mind.
hist(rowSums(fishY>0), main = "Fish - Richness") #Most samples have moderate species richness ~12 fish species per sample. Slight skew to lower richnesses but pretty normal.

hist(colMeans(invertY), nclass = 20, main = "Inverts - Mean Abun per sample unit") #Most invert species are hardly present on average, some have increased abundance to ~30 on average across sample units then some a handful (1-2) are comparatively super common. Up to 80-100 individuals per sample unit. 
hist(colMeans(invertY>0), nclass = 10, main = "Inverts - Prevalence") #Most inverts have <0.2 prevalence. Then a few more have 0.2-0.3. Beyond which it really drops off in frequency to the higher prevalence. Generally very few species are strongly prevalent.
hist(log(invertY[invertY>0]), main = "Inverts - log abundance conditional on presence") #As with fish there is still strong negative skew. Better than untransformed though.
hist(rowSums(invertY>0), main = "Inverts - Richness") #Most samples have moderate species richness ~12 inverts per sample. Slight skew to lower richnesses but pretty normal.


```

From the above we can see that both our fish and invert data have strongly positively skewed abundances (lean towards loads of 0s and few high abundances). Looks like we may need a hurdle model so that HMSC can model the presence as one component, then where the species is present we can build another model to predict the abundances.

Extract S matrix - Study design aspects.

```{r}
fishS = fishData[,c(1,3,6,7,8,9,10,11,12)] #Grab site name, sampling, year, imcra bioregion, MPA protection status (protected or not?), the MPA name the points are associated with, and the broader geographic region as study design components. We may not use all of these in the model but at least they're there. Also grab lat longs. As of 1.7.24 we also grab the location variable. This is the biounit/rough area the site is present in (PPB, The prom, etc).

invertS = invertData[, c(1,3,6,7,8,9,10,11,12)] #As above

#These are all factor variables
```

We now have our SXY matrices. Checking the names of them then moving on to actually defining models.

```{r}
str(fishS)
str(fishX)
str(fishY)

str(invertS)
str(invertX)
str(invertY)
```

### 6) Define models

Now we can begin to define some models. 

```{r}
#No need to split Xformula between inverts and fish as they get the same structure. Note the code doesnt actually call a specific object, it just holds variable names.
#Define environmental drivers.
XFormula <- ~ YSince2002 +
  Bioregion +
  adjSD + #Maybe needs to be asymtotic?
  bpi +
  #qeastness + #Variables hashed out dropped as of 13.6.2024 meeting.
  #qnorthness +
  #planc +
  profc +
  #twistc +
  Depth +
  poly(SignifWaveHeightMonthlyMean, degree = 2, raw = TRUE) + #Change to linear as of 13.6.2024 meeting???
  poly(TemperatureMonthlyMean, degree = 2, raw = TRUE)+  #Second order poly allows for intermediate thermal optimum. E.g. Some species like it warm, not hot, not cold. Can't do that with linear feature. A linear feature assumes hotter = better.
  MPAProtectStat
  
#Fish
#Define study matrix. Study type factors. Random effects not included here - dealt with later on!
#Looks like to make a random effect the associated study design variable must be included at this stage. E.g. tell hmsc it is a component of the study design, then inform it it's what we want to base our random effect on.
#Needs defining for the fish and inverts separately due to the different number of rows.

#Defining siteID as a random effect, protection as a fixed effect factor, and year as a random effect.
fishStudyDesign <- data.frame(Fyear = fishS$year, protection = fishS$MPAProtectStat, siteID = fishS$site_name, bioregion = fishS$Bioregion, biounit = fishS$location)

#Sorting out random effects
rL.fishYear <- HmscRandomLevel(units = levels(fishStudyDesign$Fyear)) #Tell hmsc that factor variable year needs to be treated as random
rL.fishSite <- HmscRandomLevel(units = levels(fishStudyDesign$siteID)) #Alternative random effect of site ID
rL.fishBioUnit <- HmscRandomLevel(units = levels(fishStudyDesign$biounit)) #Alternative random effect of biounit (the broader area - larger than a site but smaller than a bioregion)

#Define spatial random effect - fish
xy = as.matrix(cbind(fishS$longitude, fishS$latitude))
rownames(xy)= fishS$site_name #Set row names to Site ID
xy = xy[!duplicated(row.names(xy)),]#Ditch duplicate locales as we have repeat sampling
colnames(xy)=c("x-coordinate","y-coordinate")

rL.fish.nngp <- HmscRandomLevel(sData = xy, longlat = TRUE, sMethod = 'NNGP',nNeighbours = 3)
#rL.fish.nngp <- setPriors(rL.fish.nngp,nfMin=1,nfMax=2)

rL.fish.gp <- HmscRandomLevel(sData = xy, longlat = TRUE) #Default spatial method alternative

#Repeat for inverts
invertStudyDesign <- data.frame(Fyear = invertS$year, protection = invertS$MPAProtectStat, siteID = invertS$site_name, bioregion = invertS$Bioregion, biounit = invertS$location)
rL.invertYear <- HmscRandomLevel(units = levels(invertStudyDesign$Fyear)) #Tell hmsc that year needs to be treated as random
rL.invertSite <- HmscRandomLevel(units = levels(invertStudyDesign$siteID)) #Alternative random effect of site ID
rL.invertBioUnit <- HmscRandomLevel(units = levels(invertStudyDesign$biounit)) #Alternative random effect of biounit (the broader area - larger than a site but smaller than a bioregion)

xy = as.matrix(cbind(invertS$longitude, invertS$latitude))
rownames(xy)= invertS$site_name #Set row names to Site ID
xy = xy[!duplicated(row.names(xy)),]#Ditch duplicate locales as we have repeat sampling
colnames(xy)=c("x-coordinate","y-coordinate")

rL.invert.nngp <- HmscRandomLevel(sData = xy, longlat = TRUE, sMethod = 'NNGP',nNeighbours = 3)
#rL.invert.nngp <- setPriors(rL.invert.nngp, nfMin=1,nfMax=2)

rL.invert.gp <- HmscRandomLevel(sData = xy, longlat = TRUE) #Default spatial method alternative

#Define Y matrices. Needs one for pres/abs and then another to predict the abundance. Needs defining for fish and inverts
#Define for fish
fishYpa = 1*(fishY>0) #Calcs pres/abs
fishYabu = fishY
fishYabu[fishY==0] = NA
fishYabu=log(fishYabu) #Calcs log transformed abundance where presence == 1

invertYpa = 1*(invertY>0) #Calcs pres/abs
invertYabu = invertY
invertYabu[invertY==0] = NA
invertYabu=log(invertYabu) #Calcs log transformed abundance where presence == 1


```

We can also bring in a taxanomic tree for each of the fish and invert data.

```{r}
#Load in fish taxanomic data
fishTaxa = read.csv(file.path(data.directory, "FishTaxaTable.csv"), stringsAsFactors = TRUE)

#Note that this csv has all the species in it from the fish dataset. Even the odd ones where the species couldnt be ID'ed or the abundance was below our limits. Let's subset to only include rows where the species is in the fishY dataset.
fishTaxa = fishTaxa[str_c(fishTaxa$genus, fishTaxa$epithet, sep = ".") %in% names(fishY),]

str_c(fishTaxa$genus, fishTaxa$epithet, sep = ".") %>% table() #Check that we have only one entry per species. Got two for Pseudognoiistius nigripes.
fishTaxa[str_c(fishTaxa$genus, fishTaxa$epithet, sep = ".") == "Pseudogoniistius.nigripes",] #Shows we have two entries One with erroneous Family
fishTaxa = fishTaxa[!(str_c(fishTaxa$genus, fishTaxa$epithet, sep = ".") == "Pseudogoniistius.nigripes" & fishTaxa$family == "Cheilodactylidae"),]

str_c(fishTaxa$genus, fishTaxa$epithet, sep = ".") %in% names(fishY) %>% table() #Final check that all species names in tree match fishY. 58 TRUEs

#Finally add a complete species name col to the dataset
fishTaxa$species = str_c(fishTaxa$genus, fishTaxa$epithet, sep = ".") %>% as.factor()

fishTree = as.phylo(~family/genus/species,
                    data = fishTaxa,
                    collapse = FALSE) 

fishTree$edge.length = rep(1, length(fishTree$edge)) #Assume all branch lengths are equal at a value of 1

plot(fishTree, show.tip.label = FALSE,
     no.margin = TRUE)

#Repeat for inverts
invertTaxa = read.csv(file.path(data.directory, "InvertTaxaTable.csv"), stringsAsFactors = TRUE)

#Note that this csv has all the species in it from the invert dataset. Even the odd ones where the species couldnt be ID'ed or the abundance was below our limits. Let's subset to only include rows where the species is in the invertY dataset.
invertTaxa = invertTaxa[str_c(invertTaxa$genus, invertTaxa$epithet, sep = ".") %in% names(invertY),]

str_c(invertTaxa$genus, invertTaxa$epithet, sep = ".") %>% table() #Check that we have only one entry per species. Got two for Cenolia.trichoptera and two Paguroidea spp.
invertTaxa[str_c(invertTaxa$genus, invertTaxa$epithet, sep = ".") == "Cenolia.trichoptera",] #Shows we have two entries one with erroneous order
invertTaxa = invertTaxa[!(str_c(invertTaxa$genus, invertTaxa$epithet, sep = ".") == "Cenolia.trichoptera" & invertTaxa$order == "Articulata"),]
invertTaxa[str_c(invertTaxa$genus, invertTaxa$epithet, sep = ".") == "Paguroidea.spp.",] #Shows we have two entries one with no family data
invertTaxa = invertTaxa[!(str_c(invertTaxa$genus, invertTaxa$epithet, sep = ".") == "Paguroidea.spp." & is.na(invertTaxa$family) == TRUE),]

str_c(invertTaxa$genus, invertTaxa$epithet, sep = ".") %in% names(invertY) %>% table() #Final check that all species names in tree match invertY. 46 TRUEs

#Finally add a complete species name col to the dataset
invertTaxa$species = str_c(invertTaxa$genus, invertTaxa$epithet, sep = ".") %>% as.factor()

invertTree = as.phylo(~family/genus/species,
                    data = invertTaxa,
                    collapse = FALSE) 

invertTree$edge.length = rep(1, length(invertTree$edge)) #Assume all branch lengths are equal at a value of 1

plot(invertTree, show.tip.label = FALSE,
     no.margin = TRUE)



```

Finally we can define our models.

```{r}
mF1 <- Hmsc(Y= fishYpa,
            XData = fishX, XFormula = XFormula,
            #TrData = TrData, TrFormula = TrFormula,
            phyloTree = fishTree,
            distr = "probit",
            studyDesign = fishStudyDesign,
            ranLevels = list("Fyear" = rL.fishYear, #Names of random variables defined here must match cols in studydesign matrix
                             "biounit" = rL.fishBioUnit)
                             #"siteID" = rL.fish.nngp)
            )

mF2 <- Hmsc(Y= fishYabu, YScale = TRUE,
            XData = fishX, XFormula = XFormula,
            #TrData = TrData, TrFormula = TrFormula,
            phyloTree = fishTree,
            distr = "normal",
            studyDesign = fishStudyDesign,
            ranLevels = list("Fyear" = rL.fishYear,
                             "biounit" = rL.fishBioUnit)
                             #"siteID" = rL.fish.nngp)
            )

mI1 <- Hmsc(Y= invertYpa,
            XData = invertX, XFormula = XFormula,
            #TrData = TrData, TrFormula = TrFormula,
            phyloTree = invertTree,
            distr = "probit",
            studyDesign = invertStudyDesign,
            ranLevels = list("Fyear" = rL.invertYear,
                             "biounit" = rL.invertBioUnit)
                             #"siteID" = rL.invert.nngp)
            )

mI2 <- Hmsc(Y= invertYabu, YScale = TRUE,
            XData = invertX, XFormula = XFormula,
            #TrData = TrData, TrFormula = TrFormula,
            phyloTree = invertTree,
            distr = "normal",
            studyDesign = invertStudyDesign,
            ranLevels = list("Fyear" = rL.invertYear,
                             "biounit" = rL.invertBioUnit)
                             #"siteID" = rL.invert.nngp)
            )

models <- list(mF1, mF2, mI1, mI2)
modelnames <- c("fish_presence_absence", "fish_abundance_COP", "invert_presence_absence", "invert_abundance_COP")
names(models) = modelnames
saveRDS(models, file = file.path(unfitted.model.directory, "PostMeetingModelsJune24"))



```

### 7) Fit models

Now we fit the models iteratively over a loop of increasing thinning numbers. Each one takes longer than the last. But the longer ones might make better predictions.

samples = N MCMC samples to be collected from each chain.
transient = number of MCMC steps to leave as spin up. E.g. beyond this number we start using the data to get posterior samples
thin = number of MCMC steps between each recorded sample.
nChains = number of independent chains to be run
nParralel = number of parallel processes on which the chains are ran. Typically one process per chain.


```{r}
#First loop picks the model, second loop fits it across various mcmc settings then saves the model as a specifically named object.

set.seed(1995) #Repeatable randomness

#First set and forget the N chains and N samples
nChains = 4
samples = 250

for (i in 1:length(models)){
  
  print(paste("Working on model", i))
  
  focalModel = models[[i]] #Get ith model from list
  
for (thin in c(1,10)){ #100,1000))
  
  print(paste("With thinning = ", thin))
  
  transient = 50*thin
  
  m = sampleMcmc(hM = focalModel, 
                 thin = thin, 
                 samples = samples, 
                 transient = transient,
                 nChains = nChains,
                 nParallel = nChains)
  
  filename=file.path(fitted.model.directory, paste0(names(models[i]),"_model_chains_", as.character(nChains), "_samples_", as.character(samples), "_thin_", as.character(thin)))
  
  saveRDS(m, file=filename) #Save fitted model
}
}



```

### 8) Check MCMC Convergence

Now we have fitted some models on comparatively low chain lengths, let's check the convergence. If it's rubbish then there's no point looking at the parameter estimates. We grab...

- the posterior estimates of the beta parameters (the species niches)
- nothing else at present...this might change when we add phylogenetic samples or add in space. Do we need to add in estimates of omega if we aren't interested in species interactions?

```{r}
#Loop through each fitted model, read it in, grab the posterior estimates. Note we only bother doing this for the model with the most thinning (the longest chain length fitted).
highestThin = "thin_10"
highestChains = "chains_4"

fittedModels = list.files(file.path(fitted.model.directory))
fittedModels = fittedModels[grepl(highestChains, fittedModels)]
fittedModels = fittedModels[grepl(highestThin, fittedModels)]

rm(highestThin)
rm(highestChains)

for (i in 1:length(fittedModels)){
  
  #Grab each model in turn
  focalModel = readRDS(file.path(fitted.model.directory, fittedModels[i]))
  
  #What model do we have?
  print(paste("Posterior beta parameters for", fittedModels[i]))
  
  #Convert to coda
  mpost = convertToCodaObject(focalModel,
                              spNamesNumbers = c(T,F),
                              covNamesNumbers = c(T,F))
  
  #Estimate betas
  ess.beta = effectiveSize(mpost$Beta)
  psrf.beta = gelman.diag(mpost$Beta, multivariate=FALSE)$psrf
  print("Efffective Sample Size - ideally as big as possible")
  print(summary(ess.beta))
  print("Potential Scale Reduction Factor - ideally as close to 1 as possible")
  print(summary(psrf.beta))
  
  #Plot em
  hist(ess.beta)
  hist(psrf.beta)
  
  #What model do we have?
  print(paste("Posterior omega parameters for", fittedModels[i]))
  
  #Estimate Omegas - random effects
  ess.omega = effectiveSize(mpost$Omega[[1]])
  psrf.omega = gelman.diag(mpost$Omega[[1]], multivariate=FALSE)$psrf
  print("Efffective Sample Size - ideally as big as possible")
  print(summary(ess.omega))
  print("Potential Scale Reduction Factor - ideally as close to 1 as possible")
  print(summary(psrf.omega))
  
  #Plot em
  hist(ess.omega)
  hist(psrf.omega)
  
  #Grab effective spatial range estimated by spatial latent effects
  print("Estimated Spatial Range for spatial latent effect")
  summary(mpost$Alpha[[1]]) %>% print()

}


```

All four models have Beta PSRF close to 1. But not as close as in the examples in the book. Maybe we can improve model fit by respecifying? Or cranking the N chains and thinning (we can run this over a weekend).

### 9) Check explanatory and predictive power

As a final thing for these codes let's estimate the explanatory and predictive power of each model.....
- explanatory power predicts using data the model has already seen
- predictive power uses CV to test how the model goes when we hold out data and test it on the holdout sets

```{r}
#Again, looping through each model. This time we save the outputs. As with last time, we only bother with this with the longest model runs we've done in the fitted models folder.

highestThin = "thin_10"
highestChains = "chains_4"

fittedModels = list.files(file.path(fitted.model.directory))
fittedModels = fittedModels[grepl(highestChains, fittedModels)]
fittedModels = fittedModels[grepl(highestThin, fittedModels)]

rm(highestThin)
rm(highestChains)

for (i in 1:length(fittedModels)){
  
  #Grab each model in turn
  focalModel = readRDS(file.path(fitted.model.directory, fittedModels[i]))
  
  #What model do we have?
  print(paste("Explanatory and predictive power for", fittedModels[i]))
  
  #Store predictions
  preds = pcomputePredictedValues(focalModel, nParallel = nChains) #If we introduce a spatial effect we need to tweak this bit of the code to include "updater = list(GammaEta=FALSE)". Best to ask Joel before we do so
  
  #Compute explanatory power and save various metrics
  fit = evaluateModelFit(hM = focalModel, predY = preds)
  
  #If the model has TjurR2 present then give it one set of explanatory variables (those associated with a pres abs model). If it isn't then return the other set of models.
  ExplanSpSpecific = if (is.null(fit$TjurR2)){ 
    
    data.frame(Species = focalModel$spNames,
                             RMSE = fit$RMSE,
                             R2 = fit$R2)}
  else {
    
    data.frame(Species = focalModel$spNames,
                             RMSE = fit$RMSE,
                             AUC = fit$AUC,
                             TjurR2 = fit$TjurR2)
  }
  
  ExplanSpUnspecific = if (is.null(fit$TjurR2)){
    
    data.frame(Metric = c("Mean", "Median", "SD", "Min", "Max"),
                             RMSE = c(mean(fit$RMSE), median(fit$RMSE), sd(fit$RMSE), min(fit$RMSE), max(fit$RMSE)),
                             R2 = c(mean(fit$R2), median(fit$R2), sd(fit$R2), min(fit$R2), max(fit$R2)))}
  else { 
    
    data.frame(Metric = c("Mean", "Median", "SD", "Min", "Max"),
                             RMSE = c(mean(fit$RMSE), median(fit$RMSE), sd(fit$RMSE), min(fit$RMSE), max(fit$RMSE)),
                             AUC = c(mean(fit$AUC), median(fit$AUC), sd(fit$AUC), min(fit$AUC), max(fit$AUC)),
                             TjurR2 = c(mean(fit$TjurR2), median(fit$TjurR2), sd(fit$TjurR2), min(fit$TjurR2), max(fit$TjurR2)))}
  
  
  saveRDS(ExplanSpSpecific, file = file.path(localDir, "Outputs/Fit", paste("ExplanPwrSpSpecific", fittedModels[i], sep = "_")))
  saveRDS(ExplanSpUnspecific, file = file.path(localDir, "Outputs/Fit", paste("ExplanPwrSpUnspecific", fittedModels[i], sep = "_")))
  
  
  #Repeat for CV
  nfolds = 2
  partition = createPartition(focalModel, nfolds = nfolds)
  preds = pcomputePredictedValues(focalModel, partition = partition, nParallel = nChains*nfolds) #One fold from each chain on each core. So chains*folds gives cores needed.
  cvfit = evaluateModelFit(hM = focalModel, predY = preds)
  
  PredictSpSpecific = if (is.null(cvfit$TjurR2)){ 
    
    data.frame(Species = focalModel$spNames,
                             RMSE = cvfit$RMSE,
                             R2 = cvfit$R2)}
  else {
    
    data.frame(Species = focalModel$spNames,
                             RMSE = cvfit$RMSE,
                             AUC = cvfit$AUC,
                             TjurR2 = cvfit$TjurR2)
  }
  
  PredictSpUnspecific = if (is.null(cvfit$TjurR2)){
    
    data.frame(Metric = c("Mean", "Median", "SD", "Min", "Max"),
                             RMSE = c(mean(cvfit$RMSE), median(cvfit$RMSE), sd(cvfit$RMSE), min(cvfit$RMSE), max(cvfit$RMSE)),
                             R2 = c(mean(cvfit$R2), median(cvfit$R2), sd(cvfit$R2), min(cvfit$R2), max(cvfit$R2)))}
  else { 
    
    data.frame(Metric = c("Mean", "Median", "SD", "Min", "Max"),
                             RMSE = c(mean(cvfit$RMSE), median(cvfit$RMSE), sd(cvfit$RMSE), min(cvfit$RMSE), max(cvfit$RMSE)),
                             AUC = c(mean(cvfit$AUC), median(cvfit$AUC), sd(cvfit$AUC), min(cvfit$AUC), max(cvfit$AUC)),
                             TjurR2 = c(mean(cvfit$TjurR2), median(cvfit$TjurR2), sd(cvfit$TjurR2), min(cvfit$TjurR2), max(cvfit$TjurR2)))}
  
  saveRDS(PredictSpSpecific, file = file.path(localDir, "Outputs/Fit", paste("PredictPwrSpSpecific", fittedModels[i], sep = "_")))
  saveRDS(PredictSpUnspecific, file = file.path(localDir, "Outputs/Fit", paste("PredictPwrSpUnspecific", fittedModels[i], sep = "_")))
}



```

Optional chunk below for re-reading in those dataframes we just made and exports as csv. Useful for mucking around with with others.

```{r}
fitMetrics = list.files(file.path(localDir, "/Outputs/Fit"))

highestThin = "thin_10"
highestChains = "chains_4"

fitMetrics = fitMetrics[grepl(highestChains, fitMetrics)]
fitMetrics = fitMetrics[grepl(highestThin, fitMetrics)]
fitMetrics = fitMetrics[!grepl(".xls", fitMetrics)]
fitMetrics = fitMetrics[!grepl(".csv", fitMetrics)]

rm(highestThin)
rm(highestChains)

#Looped version. Non-species specific ones are a good guide barometer of fit. Loads in then saves as csv
for (i in 1:length(fitMetrics)){
  temp = readRDS(file = file.path(localDir, "/Outputs/Fit",fitMetrics[i]))
  
  write.csv(temp, file = file.path(localDir, "Outputs/Fit", paste(fitMetrics[i], ".csv", sep = "")), row.names = FALSE)
    

}




```

We now have species specific, and overall, measures of model fit we can use to assess the models.

In the next set of codes we'll look at the estimates of each parameter to see what is driving this model fit. And begin to look at trends between the response variables and their predictors.

