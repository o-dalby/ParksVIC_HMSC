---
title: "1-DataImportAndWrangling"
author: "Oliver Dalby"
date: "`r Sys.Date()`"
output: html_document
---

### Purpose

Purpose of the codes is to ingest, format, wrangle, and amalgamate the resposne and predictor data needed for the Parks VIC HMSC project. 
This is a set up chunk of codes that ideally will be ran once and then the derivatives from it can be used in the future without needing to re-run.

The overall aim of the project is to see if we can use joint species distribution models (JSDMs) to model the role that Victoria's marine parks have on the fish and invert communities present and see if we can identify environmental and biological factors which influence community composition. Our measure of success for the parks is to see if the communities are noticeably more complex and abundant in the park vs in adjacent reference areas. JSDMs are useful for this as we can predict the whole communities response (both to environmental variables, latent effects like space and time, and to biological interactions).

This work is being undertaken in collaboration with Michael Sams (Park's VIC), UTAS (Joel Williams) and Deakin Uni (Mary Young, Oli Dalby).

Fish and Invert data are taken from a long term dataset from Reff Life Survey spanning ~30 years of data (1992-2023).

### Package import

```{r}
library(terra)
library(collapse)
library(sf)
library(MultiscaleDTM) #For terrain metrics https://doi.org/10.1111/tgis.13067
library(parallel)
library(memuse)
library(tmap)
library(tidyverse)
library(ncdf4)
library(INLA)
```

### 1) Bathymetry and terrain derivatives

One key predictor of fish and invert communities is the bathymetry (water depth) but also the terrain present. There are lots of metrics available to assess this so we can use a new package by Canadian colleagues.

First we take existing 2.5m res bathy for the state, combine it into one statewide projection from two zonal ones, resample to 5m res. Then we can generate a 50m downsampled version of this bathymetry as our template raster for the study and can compute the relevant terrain metrics. 

```{r}
#Initial muck around with terra options to let it use lots of our available RAM
terraOptions() #Show options we're working with by default
memuse::Sys.meminfo() #Show RAM we have to work with too

#Give terra up to 85% of RAM
terraOptions(memfrac = 0.85)
terraOptions()


```

```{r}
#Set WD for layers we make
setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")

#Import bathy for entire state. Extract bounding boxes and reproj these shapefiles to VICGrid

#VICBathyZ54 = rast("Z:/GIS-Res/DELWP/Veris_VICDEM2022_Delivery/VCDEM2021_Tiff/VCDEM_VicGrid2020_10m_Tiff/VCDEM2021_GDA2020_z54_Seamless.tif") %>% ext() %>% as.polygons(crs = "epsg:7854") %>% project("epsg:7899") %>% st_as_sf()

#VICBathyZ55 = rast("Z:/GIS-Res/DELWP/Veris_VICDEM2022_Delivery/VCDEM2021_Tiff/VCDEM_VicGrid2020_10m_Tiff/VCDEM2021_GDA2020_z55_Seamless.tif") %>% ext() %>% as.polygons(crs = "epsg:7855") %>% project("epsg:7899") %>% st_as_sf()

#Mary has assured me that the two rasters were originally one, so we dont need to worry about the differences in bathymetry values where they overlap. They've already undergone standardisation processes before us.

#Identify maximal extent of the rasters based on their combined reprojected extents
#temp = st_union(VICBathyZ54, VICBathyZ55) %>% vect()
#plot(temp) #Quick plot to make sure it doesnt look mental

#Make template raster in VICGrid for the entire extent of the two rasters. This way we reproject each one onto the same grid to ensure cell size and origin are identical.
#Also make the new template at 5m. This is the same as the res Mary sent us initially in the latyer that didn't work.
#Template = rast(xmin = ext(temp)[1],
#              xmax = ext(temp)[2],
#              ymin = ext(temp)[3],
#              ymax = ext(temp)[4],
#              crs = "epsg:7899",
#              resolution = 5)
#rm(temp)

#Reproject both onto the new template grid.
#VICBathyZ54 = rast("Z:/GIS-Res/DELWP/Veris_VICDEM2022_Delivery/VCDEM2021_Tiff/VCDEM_VicGrid2020_10m_Tiff/VCDEM2021_GDA2020_z54_Seamless.tif") %>%
#  project(Template)

#VICBathyZ55 = rast("Z:/GIS-Res/DELWP/Veris_VICDEM2022_Delivery/VCDEM2021_Tiff/VCDEM_VicGrid2020_10m_Tiff/VCDEM2021_GDA2020_z55_Seamless.tif") %>%
#  project(Template)

#Now merge
#origin(VICBathyZ54) == origin(VICBathyZ55) #Check origins match
#res(VICBathyZ54) == res(VICBathyZ55) #Check res match

#VICBathy5m = list(VICBathyZ54, VICBathyZ55) %>% sprc() %>% #Make spatraster collection
#  terra::merge(filename = "VICBathy5mMerged.tif", overwrite = TRUE) 

#Quick print of bathy details
#VICBathy5m %>% print()

#Clip and mask the bathy layer to only include areas from the coastline to sea. gets rid of lots of cells on land we dont care about.
#coastlinePoly = st_read("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VIC_coastalwaters_mask.shp") %>% st_transform(crs = "EPSG:7899") %>% vect()
#setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")
#names(VICBathy5m) = "VCDEM2021"
#VICBathy5m = VICBathy5m %>% mask(coastlinePoly) %>% crop(coastlinePoly,
#                                                         filename = "VICBathy5mCoastClipped.tif",
#                                                         overwrite = TRUE)

#Convert bathy layer to 50m res. This will form the template raster for the study. Calcing mean and SE.
#setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")
#VICBathy50mMean = VICBathy5m %>% aggregate(fact = 10, fun = "mean", na.rm = TRUE, filename = "VICBathy50mMean.tif", overwrite = TRUE, cores = 5)
#VICBathy50mSD = VICBathy5m %>% aggregate(fact = 10, fun = "sd", na.rm = TRUE, filename = "VICBathy50mSD.tif", overwrite = TRUE, cores = 5)

#Re-import bathymetry as we have already ran the above in previous days.
VICBathy5m = rast("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VICBathy5mCoastClipped.tif")

#Compute terrain matrices at 45m neighbourhood scale (9 cells in the 5m raster). Scale chosen as this is the scale at which RSL transects are undertaken (they are 50m in length).
#Note we use the original bathy layer for this not the coarsened one.

subsets = VICBathy5m %>% ext() %>% as.polygons(crs = crs(VICBathy5m)) %>% 
  split(f = st_read("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/Splits.shp") %>% vect())

#Calculate single metrics as computing them all at returns fatal error. Splitting raster into subsets as computing on the full layer, even with one metric, also breaks R.
setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")

for (i in 1:length(subsets)){

print(str_c("Working on Subset ", i))  

print("Getting cropped raster")
subsetBuff1 = subsets[i] %>% terra::buffer(width = 250) #Take subset and make buffer to reduce edge effects. 250m wide to ensure we dont impact edge values.

rastSubset = VICBathy5m %>% mask(subsetBuff1) %>% crop(subsetBuff1) #Subset raster using buffered subset polygon. Calculating each terrain metric on one subset at a time.

subsetBuff2 = subsets[i] %>% terra::buffer(width = 10) #Make another buffer thats a smidge larger than our raster to ensure that we are including all the data we want in our eventual merges. Dont want to introduce holes into the rasters we make!

print("Calculating Slope")
Slope = Qfit(rastSubset,
             na.rm = TRUE,
             metrics = c("qslope")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("SlopeSubset", i, ".tif"), overwrite = TRUE) #Drop any cells associated with the buffer we provided now we have our terrain metric

rm(Slope)
gc()

print("Calculating Northness")
Northness = Qfit(rastSubset,
             w = c(9,9),
             na.rm = TRUE,
             metrics = c("qnorthness")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("NorthnessSubset", i, ".tif"), overwrite = TRUE)

rm(Northness)
gc()

print("Calculating Eastness")
Eastness = Qfit(rastSubset,
             w = c(9,9),
             na.rm = TRUE,
             metrics = c("qeastness")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("EastnessSubset", i, ".tif"), overwrite = TRUE)

rm(Eastness)
gc()

print("Calculating Profile Curvature")
Profc = Qfit(rastSubset,
             w = c(9,9),
             na.rm = TRUE,
             metrics = c("profc")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("ProfcSubset", i, ".tif"), overwrite = TRUE)

rm(Profc)
gc()

print("Calculating Calculating Planar Curvature")
Planc = Qfit(rastSubset,
             w = c(9,9),
             na.rm = TRUE,
             metrics = c("planc")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("PlancSubset", i, ".tif"), overwrite = TRUE)

rm(Planc)
gc()

print("Calculating Twisting Curvature")
Twistc = Qfit(rastSubset,
             w = c(9,9),
             na.rm = TRUE,
             metrics = c("twistc")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("TwistcSubset", i, ".tif"), overwrite = TRUE)

rm(Twistc)
gc()

print("Calculating BPI")
BPI = BPI(rastSubset,
             w = c(9,9),
             na.rm = TRUE) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("BPISubset", i, ".tif"), overwrite = TRUE)

rm(BPI)
gc()

print("Calculating Adjusted SD")
AdjSD = AdjSD(rastSubset,
             w = c(9,9),
             na.rm = TRUE) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("AdjSDSubset", i, ".tif"), overwrite = TRUE)

rm(AdjSD)
gc()

print("Calculating VRM")
VRM = VRM(rastSubset,
             w = c(9,9),
             na.rm = TRUE) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("VRMSubset", i, ".tif"), overwrite = TRUE)

rm(VRM)
gc()

}

#Take all the subsets for each metric calculated and merge them into one.
metrics = c("Slope", "Northness", "Eastness", "Profc", "Planc", "Twistc", "BPI", "AdjSD", "VRM")

setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")

for (i in 1:length(metrics)) {
  
  #Import all the rasters from a given metric and store as sprc. Then merge them back to one large raster. Becuase we use such large overlaps in our buffers the overlaps present between the rasters here contain identical values.
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")
  stack = list.files() %>% 
    str_subset(pattern = str_c(metrics[i], "Subset")) %>% 
    sprc() %>% 
    merge(filename = str_c(metrics[i], "Statewide.tif"), 
            overwrite = TRUE)
  
}


#Also aggregate the terrain metrics to 50m res so they match the bathy.
for (i in 1:length(metrics)) {
  
  #Import all the rasters from a given metric and then export to 50m res using the mean cell value.
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")
  raster = list.files() %>% 
    str_subset(pattern = str_c(metrics[i], "Statewide.tif")) %>% 
    rast() %>% 
    aggregate(fact = 10,
              fun = "mean",
              na.rm = TRUE,
              filename = str_c(metrics[i], "Statewide50m.tif"), 
              overwrite = TRUE)
  
}


```

### 2) Wave data

Next off, import Wave data from Jin Liu (DEECA). These are models of the wave climate of victoria from https://doi.org/10.1016/j.renene.2023.118943 and https://doi.org/10.1016/j.ocemod.2022.101980. We will import all available data for the years 1992 through 2023, calculate the daily average (mean) and then compute an annual average and SD and a winter average and 95% CI. This will be completed for significant wave height, partitioned wind wave, partitioned primary and secondary swells, peak wave frequency, mean wave freq (1/mean wave period). The idea behind using averages and seasonal measures is to capture the general conditions species present are exposed to and to also capture the season that is likely to be most stressful for them (in this case, winter).

Wave variables and their meanings:
Signif wave height = average height of the largest 1/3 of the waves. E.g. How big are the biggest waves?
Peak Wave Freq = frequency (Hz) of the highest energy waves. E.g. how common are the big waves?
Mean Wave Frequency = average freq of all waves in the unit time.
Wind Wave = swell of waves originating from local wind forces.
Primary and secondary swell heights = Primary swells occurs from far offshore and are normally larger, longer periods, and higher amplitudes. Secondaries are smaller, weaker.

Data are in the form of one netcdf for each variable and year combination. So lots of looping is required here. Data available only span 1981 to 2020 so we will have to upper bound our analyses at 2020. We can still lower bound at 1992


```{r}
#Prep work
directoriesList = c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/SignifWaveHeight",
                    "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/PeakWaveFreq",
                    "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/MeanWavePeriod",
                    "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/PartitionedWindWave",
                    "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/PartitionedPrimarySwell",
                    "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/PartitionedSecondarySwell")

years = seq(1992, 2020, 1) %>% as.character()

metrics = c("hs", "fp", "t02", "phs0", "phs1", "phs2")

#Start outer loop here. Controls the variable we are working with.
for (j in 1:length(directoriesList)){

print(str_c("Currently working on", " ", metrics[j]))

files = map(years, str_subset, string = directoriesList[j] %>% list.files()) %>% reduce(union) #Map function to the rescue!

#Connect to first netcdf and extract matrix dimensions for Lat Lon.
data = nc_open(str_c(directoriesList[j], "/", files[1])) #Only get first slice of data

#Make holder arrays. Array as above but with N layers where N = number of years we're working with. Need one per variable we want to extract.
lat = ncvar_get(data, "latitude") #Get node lat
lon = ncvar_get(data, "longitude") #Get node lon
annualMean = matrix(data = NA, nrow = length(lat), ncol = length(years)) #Rows = number of nodes, Cols = N years assessed
annualSD = matrix(data = NA, nrow = length(lat), ncol = length(years)) #Rows = number of nodes, Cols = N years assessed
winterMean = matrix(data = NA, nrow = length(lat), ncol = length(years)) #Rows = number of nodes, Cols = N years assessed
winterSD = matrix(data = NA, nrow = length(lat), ncol = length(years)) #Rows = number of nodes, Cols = N years assessed
winter95th = matrix(data = NA, nrow = length(lat), ncol = length(years)) #Rows = number of nodes, Cols = N years assessed

#Close connection from initial netcdf4 access
nc_close(data)

#From here on the values change based on the file we read in so we need to change them iteratively over loops. Inner loop.
for (i in 1:length(files)) {
  
print(str_c("Completing loop", " ", i))

data = nc_open(str_c(directoriesList[j], "/", files[i]))

date = ncvar_get(data, "time") %>% as.Date(origin = "1990-01-01 00:00:00", tz = "Australia/Victoria") %>% as.character() #Grab date of each time slice. 24 slices per day (so each slice is an hour).

fullArray = ncvar_get(data, metrics[j]) #Data are in long form. 97276 nodes (not in lat long - rows) and time (number of time entries varies between years - cols).

dailyMean = fullArray  %>% t() %>% data.frame() #Transpose so days are now rows

dailyMean = dailyMean %>% 
  mutate(date = date) %>% #Add grouping variable that is date
  fgroup_by(date) %>% #Using f functions as they're super fast versions of the normal dplyr stuff
  fmean(na.rm = TRUE) %>% #Calc daily mean
  mutate(month = month(date)) #Append new month column

#We can now calculate our annual measures (mean and SD). Returns vectors of values which can be slotted straight into holders.
annualMean[,i] = dailyMean %>% 
  dplyr::select(!c(date, month)) %>% 
  fmean(na.rm = TRUE)

annualSD[,i] = dailyMean %>% 
  dplyr::select(!c(date, month)) %>% 
  fsd(na.rm = TRUE)

#And our seasonal measures - wave data want winter mean, SD, 95th. Returns vectors of values which can be slotted straight into holders.
winterMean[,i] = dailyMean %>% 
  dplyr::select(!date) %>%
  filter(month %in% c(7,8,9)) %>% 
  dplyr::select(!month) %>% 
  fmean()

winterSD[,i] = dailyMean %>% 
  dplyr::select(!date) %>%
  filter(month %in% c(7,8,9)) %>% 
  dplyr::select(!month) %>% 
  fsd()

temp = dailyMean %>% #Quantiles function cant take dataframes so we need to loop again
  dplyr::select(!date) %>%
  filter(month %in% c(7,8,9)) %>% 
  dplyr::select(!month)

winter95th[,i] = sapply(temp, FUN = quantile, probs = c(0.95), na.rm = TRUE)

#Drop objects to save RAM, disconnect from netcdf, then close loop after final iteration
nc_close(nc = data)
rm(date, fullArray, dailyMean, data)

}

#Now the variable is done we can export the matrices as separate R files to save RAM.
saveRDS(annualMean,
        str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
             directoriesList[j] %>% str_split(pattern = "/") %>% unlist() %>% dplyr::last(),
             "AnnualMean"))

saveRDS(annualSD,
        str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
             directoriesList[j] %>% str_split(pattern = "/") %>% unlist() %>% dplyr::last(),
             "AnnualSD"))

saveRDS(winterMean,
        str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
             directoriesList[j] %>% str_split(pattern = "/") %>% unlist() %>% dplyr::last(),
             "WinterMean"))

saveRDS(winterSD,
        str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
             directoriesList[j] %>% str_split(pattern = "/") %>% unlist() %>% dplyr::last(),
             "WinterSD"))

saveRDS(winter95th,
        str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
             directoriesList[j] %>% str_split(pattern = "/") %>% unlist() %>% dplyr::last(),
             "Winter95th"))

#Drop objects we'll remake at the start
rm(annualMean, annualSD, winterMean, winterSD, winter95th)

#Close outer loop
}

```

We have the frequency for the peak wave data but the period for the mean wave data. Let's do a quick loop and remedy that and convert the mean wave period to the mean wave freq.


```{r}
files = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves") %>% 
  str_subset(pattern = "Period")

for (i in 1:length(files)){
  
  data = readRDS(str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
             files[i]))
  
  data = 1/data #Convert period to frequency
  
  saveRDS(data,
          str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
                files[i]) %>%
            str_replace(pattern = "Period", replacement = "Freq"))
}

```

Now, let's convert our datasets into XYZ format so they're easier to work with in a spatial setting.

```{r}
#Grab Lat and Long data from original netcdf. These are in a worldwide projection EPSG4326.
data = nc_open("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/MeanWaveDirection/ww3.hindcast_1981_dir.nc") #Just an example nc, they all have identical dimensions
latitude = ncvar_get(data, "latitude") #%>% data.frame() %>% rename(latitude = names(.)[1])
longitude = ncvar_get(data, "longitude") #%>% data.frame() %>% rename(longitude = names(.)[1])
nc_close(data)

#Convert the location data to an sf object so we can get spatial. 
#MAJOR NOTE!!! These data are already in column form (i.e. we dont need to expand the grid as we do the temp data). We can just bind them as columns.
XYsf = data.frame(longitude = longitude,
                  latitude = latitude) %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% 
  mutate(longitude = st_coordinates(.)[,1],
         latitude = st_coordinates(.)[,2]) %>% 
  st_drop_geometry()

#Import the data we wish to interpolate.
directoriesList = "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/"
files = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves")
yearsAnnual = seq(1992, 2020)

#Loop through each layer of the array and add the years data as a separate column
for (j in 1:length(files)) {
  
  print(str_c("Working on variable ", files[j]))

  #Read in array values
  arrayValues = readRDS(str_c(directoriesList[1],
    files[j]))
  
  #Make a copy to muck around with in the inner loop
  Holder = XYsf

  for (i in (1:dim(arrayValues)[2])){
  
    print(str_c("Processing slice ", i))
  
    Holder[[i+2]] = arrayValues[,i]
  }

  #Rename Columns
  names(Holder)[3:length(names(Holder))] = yearsAnnual[3:ncol(Holder)-2]
  
  #Save
  saveRDS(Holder,
          str_c(directoriesList[1],
                files[j],
                "XYZ"))
}



```

### 3) Temperature Data

Now we've got the wave predictors wrangled and exported (but notably not interpolated quite yet) we can rinse and repeat for the temperature data. This data is sourced from the AODN (Australian Ocean Data Network). Data are monthly means collated from daily data. Data are NOAA Daily Optimum Interpolation Sea Surface Temperature originally but aggregated to monthly means. Relevant papers are Reynolds, et al.(2007) https://doi.org/10.1175/2007JCLI1824.1), Banzon, et al.(2016) https://doi.org/10.5194/essd-8-165-2016), Huang, B., C. Liu, V. Banzon, E. Freeman, G. Graham, B. Hankins, T. Smith, and H.-M. Zhang, 2021 https://doi.org/10.1175/JCLI-D-20-0166.1). Data links: 

Description.....https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00680.
Download......https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html


Shout out to Dr Alex Shute for handling the data export for these variables.

Generally we can take the same steps as we did above for each of the wave variables. We want the mean and SD annually and the summer mean, sd, and 95th percentile.

```{r}
#Investigate the netcdfs first
directoriesList = c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Temperature")

files = list.files(directoriesList) %>% str_subset(pattern = ".nc")#Map function to the rescue!

#First dataset 
data = nc_open(str_c(directoriesList, "/", files[2]))

data

date = ncvar_get(data, "time") %>% as.Date(origin = "1800-01-01 00:00:00", tz = "Australia/Victoria") %>% as.character()

print(date[1]) #Time starts at 1981-09-01
print(date[length(date)-1]) #And ends at 2024-03-01

rm(date) #Don't need this level of detail as we are working with annual means and monthly data

yearmonth = ncvar_get(data, "time") %>% as.Date(origin = "1800-01-01 00:00:00", tz = "Australia/Victoria") %>% format("%Y-%m") %>% as.character()
year = ncvar_get(data, "time") %>% as.Date(origin = "1800-01-01 00:00:00", tz = "Australia/Victoria") %>% format("%Y") %>% as.character()
month = ncvar_get(data, "time") %>% as.Date(origin = "1800-01-01 00:00:00", tz = "Australia/Victoria") %>% format("%m") %>% as.character()

#Load in full array of all data
fullArray = ncvar_get(data, "sst")
dim(fullArray) #Long, Lat, Time

#Calculate Annual Mean and SD

AnnualMeanHolder = array(data = NA, dim = c(108, 80, year %>% unique() %>% length()))
AnnualSDHolder = array(data = NA, dim = c(108, 80, year %>% unique() %>% length()))

for (i in 1:length(unique(year))){
  
  print(str_c("Calculting year ", i))
  
  subset = fullArray[, , min(which(year == unique(year)[i])) : max(which(year == unique(year)[i]))] #Load ith year of data in
  
  AnnualMeanHolder[, , i] = apply(subset, MARGIN = c(1,2), FUN = "mean", na.rm = TRUE) #Calc mean and store in ith layer
  AnnualSDHolder[, , i] = apply(subset, MARGIN = c(1,2), FUN = "sd", na.rm = TRUE) #Calc se and store in ith layer
  
}

#Export Annual Measures
saveRDS(AnnualMeanHolder,
        "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/TemperatureAnnualMean")

saveRDS(AnnualSDHolder,
        "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/TemperatureAnnualSD")


#Repeat for Summer equivalents - Dec -> Feb. Trickier as Dec is year X and Jan-Feb is YearX+1 but they need to sit together before being averaged.
Summeryearmonth = yearmonth[month %>% as.numeric() %in% c(12, 1, 2)]

summerArray = fullArray[, , which(yearmonth %in% Summeryearmonth)] #Retain only summer months from full array

SummerMeanHolder = array(data = NA, dim = c(108, 80, year %>% unique() %>% length()-1))
SummerSDHolder = array(data = NA, dim = c(108, 80, year %>% unique() %>% length()-1))
Summer95thHolder = array(data = NA, dim = c(108, 80, year %>% unique() %>% length()-1))

for (i in 1:(dim(summerArray)[3]/3)){ #Dividing by 3 gives us the number of summer period (43). One less than the years we have (44) as summer spans two years each year.
  
  print(str_c("Calculting year ", i))
  
  subset = summerArray[ , , (((i-1)*3)+1) : (((i-1)*3)+2)] #Load ith year of data in. Rows 1-3, 4-6, 7-9, 10-12, etc.
  
  SummerMeanHolder[, , i] = apply(subset, MARGIN = c(1,2), FUN = "mean", na.rm = TRUE) #Calc mean and store in ith layer
  SummerSDHolder[, , i] = apply(subset, MARGIN = c(1,2), FUN = "sd", na.rm = TRUE) #Calc se and store in ith layer
  SummerSDHolder[, , i] = apply(subset, MARGIN = c(1,2), FUN = "quantile", na.rm = TRUE, probs = c(0.95)) #Calc se and store in ith layer

}


#Export Annual Measures
saveRDS(SummerMeanHolder,
        "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/TemperatureSummerMean")

saveRDS(SummerSDHolder,
        "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/TemperatureSummerSD")

saveRDS(Summer95thHolder,
        "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/TemperatureSummer95th")


```

Like we did for the wave data, let's now change our data into an XYZ format. This will make things easier for our interpolations.

```{r}
#Grab Lat and Long data from original netcdf. These are in a worldwide projection EPSG4326.
data = nc_open("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Temperature/noaa_sst_masked_southeast_aus.nc")
latitude = data$dim$lat$vals #%>% data.frame() %>% rename(latitude = names(.)[1])
longitude = data$dim$lon$vals #%>% data.frame() %>% rename(longitude = names(.)[1])
nc_close(data)

#Convert the location data to an sf object so we can get spatial.
XYsf = base::expand.grid(longitude, latitude) %>% 
  rename(latitude = names(.)[2],
         longitude = names(.)[1]) %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% 
  mutate(longitude = st_coordinates(.)[,1],
         latitude = st_coordinates(.)[,2]) %>% 
  st_drop_geometry()

#Import the data we wish to interpolate.
directoriesList = "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/"
files = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature")
yearsAnnual = seq(1981, 2024)

#Loop through each layer of the array and add the years data as a separate column
for (j in 1:length(files)) {
  
  print(str_c("Working on variable ", files[j]))

  #Read in array values
  arrayValues = readRDS(str_c(directoriesList[1],
    files[j]))
  
  #Make a copy to muck around with in the inner loop
  Holder = XYsf

  for (i in (1:dim(arrayValues)[3])){
  
    print(str_c("Processing slice ", i))
  
    Holder[[i+2]] = arrayValues[,,i] %>% matrix(ncol = 1, byrow = FALSE)
  }

  #Rename Columns
  names(Holder)[3:length(names(Holder))] = yearsAnnual[3:ncol(Holder)-2]
  
  #Save
  saveRDS(Holder,
          str_c(directoriesList[1],
                files[j],
                "XYZ"))
}

```


### 4) Interpolations

Now we need to interpolate the results of the temperature and wave data arrays to te template raster. 

Beginning with temperature data as it's the smaller set of data. The inla process can be quite complex....be warned.

```{r}
#Guidance from this link
#https://rpubs.com/jafet089/886687
#Another excellent INLA resource
#https://ourcodingclub.github.io/tutorials/inla/
#Another one
#https://www.paulamoraga.com/book-geospatial/sec-inla.html

#Import the bathymetry layer we are using as our template
templateRast = rast("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VICBathy5m.tif")






```