---
title: "1-DataImportAndWrangling"
author: "Oliver Dalby"
date: "`r Sys.Date()`"
output: html_document
---

### Purpose

Purpose of the codes is to ingest, format, wrangle, and amalgamate the resposne and predictor data needed for the Parks VIC HMSC project. 
This is a set up chunk of codes that ideally will be ran once and then the derivatives from it can be used in the future without needing to re-run.

The overall aim of the project is to see if we can use joint species distribution models (JSDMs) to model the role that Victoria's marine parks have on the fish and invert communities present and see if we can identify environmental and biological factors which influence community composition. Our measure of success for the parks is to see if the communities are noticeably more complex and abundant in the park vs in adjacent reference areas. JSDMs are useful for this as we can predict the whole communities response (both to environmental variables, latent effects like space and time, and to biological interactions).

This work is being undertaken in collaboration with Michael Sams (Park's VIC), UTAS (Joel Williams) and Deakin Uni (Mary Young, Oli Dalby).

Fish and Invert data are taken from a long term dataset from Reff Life Survey spanning ~30 years of data (1992-2023).

### Package import

```{r}
library(terra)
library(collapse)
library(sf)
library(MultiscaleDTM) #For terrain metrics https://doi.org/10.1111/tgis.13067
library(parallel)
library(memuse)
library(tmap)
library(tidyverse)
library(ncdf4)
library(INLA)
library(raster)
library(GVI) #Needs installing using github remote installation as package not on cran at time of writing
library(BalancedSampling)
```

### 1) Bathymetry and terrain derivatives

One key predictor of fish and invert communities is the bathymetry (water depth) but also the terrain present. There are lots of metrics available to assess this so we can use a new package by Canadian colleagues.

First we take existing 2.5m res bathy for the state, combine it into one statewide projection from two zonal ones, resample to 5m res. Then we can generate a 50m downsampled version of this bathymetry as our template raster for the study and can compute the relevant terrain metrics. 

```{r}
#Initial muck around with terra options to let it use lots of our available RAM
terraOptions() #Show options we're working with by default
memuse::Sys.meminfo() #Show RAM we have to work with too

#Give terra up to 85% of RAM
terraOptions(memfrac = 0.85)
terraOptions()


```

```{r, eval = FALSE}
#Set WD for layers we make
setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")

#Import bathy for entire state. Extract bounding boxes and reproj these shapefiles to VICGrid

#VICBathyZ54 = rast("Z:/GIS-Res/DELWP/Veris_VICDEM2022_Delivery/VCDEM2021_Tiff/VCDEM_VicGrid2020_10m_Tiff/VCDEM2021_GDA2020_z54_Seamless.tif") %>% ext() %>% as.polygons(crs = "epsg:7854") %>% project("epsg:7899") %>% st_as_sf()

#VICBathyZ55 = rast("Z:/GIS-Res/DELWP/Veris_VICDEM2022_Delivery/VCDEM2021_Tiff/VCDEM_VicGrid2020_10m_Tiff/VCDEM2021_GDA2020_z55_Seamless.tif") %>% ext() %>% as.polygons(crs = "epsg:7855") %>% project("epsg:7899") %>% st_as_sf()

#Mary has assured me that the two rasters were originally one, so we dont need to worry about the differences in bathymetry values where they overlap. They've already undergone standardisation processes before us.

#Identify maximal extent of the rasters based on their combined reprojected extents
#temp = st_union(VICBathyZ54, VICBathyZ55) %>% vect()
#plot(temp) #Quick plot to make sure it doesnt look mental

#Make template raster in VICGrid for the entire extent of the two rasters. This way we reproject each one onto the same grid to ensure cell size and origin are identical.
#Also make the new template at 5m. This is the same as the res Mary sent us initially in the latyer that didn't work.
#Template = rast(xmin = ext(temp)[1],
#              xmax = ext(temp)[2],
#              ymin = ext(temp)[3],
#              ymax = ext(temp)[4],
#              crs = "epsg:7899",
#              resolution = 5)
#rm(temp)

#Reproject both onto the new template grid.
#VICBathyZ54 = rast("Z:/GIS-Res/DELWP/Veris_VICDEM2022_Delivery/VCDEM2021_Tiff/VCDEM_VicGrid2020_10m_Tiff/VCDEM2021_GDA2020_z54_Seamless.tif") %>%
#  project(Template)

#VICBathyZ55 = rast("Z:/GIS-Res/DELWP/Veris_VICDEM2022_Delivery/VCDEM2021_Tiff/VCDEM_VicGrid2020_10m_Tiff/VCDEM2021_GDA2020_z55_Seamless.tif") %>%
#  project(Template)

#Now merge
#origin(VICBathyZ54) == origin(VICBathyZ55) #Check origins match
#res(VICBathyZ54) == res(VICBathyZ55) #Check res match

#VICBathy5m = list(VICBathyZ54, VICBathyZ55) %>% sprc() %>% #Make spatraster collection
#  terra::merge(filename = "VICBathy5mMerged.tif", overwrite = TRUE) 

#Quick print of bathy details
#VICBathy5m %>% print()

#Clip and mask the bathy layer to only include areas from the coastline to sea. gets rid of lots of cells on land we dont care about.
#coastlinePoly = st_read("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VIC_coastalwaters_mask.shp") %>% st_transform(crs = "EPSG:7899") %>% vect()
#setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")
#names(VICBathy5m) = "VCDEM2021"
#VICBathy5m = VICBathy5m %>% mask(coastlinePoly) %>% crop(coastlinePoly,
#                                                         filename = "VICBathy5mCoastClipped.tif",
#                                                         overwrite = TRUE)

#Convert bathy layer to 50m res. This will form the template raster for the study. Calcing mean and SE.
#setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")
#VICBathy50mMean = VICBathy5m %>% aggregate(fact = 10, fun = "mean", na.rm = TRUE, filename = "VICBathy50mMean.tif", overwrite = TRUE, cores = 5)
#VICBathy50mSD = VICBathy5m %>% aggregate(fact = 10, fun = "sd", na.rm = TRUE, filename = "VICBathy50mSD.tif", overwrite = TRUE, cores = 5)

#Re-import bathymetry as we have already ran the above in previous days.
VICBathy5m = rast("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VICBathy5mCoastClipped.tif")

#Compute terrain matrices at 45m neighbourhood scale (9 cells in the 5m raster). Scale chosen as this is the scale at which RSL transects are undertaken (they are 50m in length).
#Note we use the original bathy layer for this not the coarsened one.

#subsets = VICBathy5m %>% ext() %>% as.polygons(crs = crs(VICBathy5m)) %>% 
#  split(f = st_read("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/Splits.shp") %>% vect())

#Subsetting layers generated from polygon splitter tool in QGIS used on a polygon of the raster extent
subsets = st_read("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/TerrainMetricsSubsets.shp") %>% vect()

#Calculate single metrics as computing them all at returns fatal error. Splitting raster into subsets as computing on the full layer, even with one metric, also breaks R.
setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")

for (i in 1:length(subsets)){

print(str_c("Working on Subset ", i))  

print("Getting cropped raster")
subsetBuff1 = subsets[i] %>% terra::buffer(width = 250) #Take subset and make buffer to reduce edge effects. 250m wide to ensure we dont impact edge values.

rastSubset = VICBathy5m %>% mask(subsetBuff1) %>% crop(subsetBuff1) #Subset raster using buffered subset polygon. Calculating each terrain metric on one subset at a time.

subsetBuff2 = subsets[i] %>% terra::buffer(width = 10) #Make another buffer thats a smidge larger than our raster to ensure that we are including all the data we want in our eventual merges. Dont want to introduce holes into the rasters we make!

print("Calculating Slope")
Slope = Qfit(rastSubset,
             na.rm = TRUE,
             metrics = c("qslope")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("SlopeSubset", i, ".tif"), overwrite = TRUE) #Drop any cells associated with the buffer we provided now we have our terrain metric

rm(Slope)
gc()

print("Calculating Northness")
Northness = Qfit(rastSubset,
             w = c(9,9),
             na.rm = TRUE,
             metrics = c("qnorthness")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("NorthnessSubset", i, ".tif"), overwrite = TRUE)

rm(Northness)
gc()

print("Calculating Eastness")
Eastness = Qfit(rastSubset,
             w = c(9,9),
             na.rm = TRUE,
             metrics = c("qeastness")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("EastnessSubset", i, ".tif"), overwrite = TRUE)

rm(Eastness)
gc()

print("Calculating Profile Curvature")
Profc = Qfit(rastSubset,
             w = c(9,9),
             na.rm = TRUE,
             metrics = c("profc")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("ProfcSubset", i, ".tif"), overwrite = TRUE)

rm(Profc)
gc()

print("Calculating Calculating Planar Curvature")
Planc = Qfit(rastSubset,
             w = c(9,9),
             na.rm = TRUE,
             metrics = c("planc")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("PlancSubset", i, ".tif"), overwrite = TRUE)

rm(Planc)
gc()

print("Calculating Twisting Curvature")
Twistc = Qfit(rastSubset,
             w = c(9,9),
             na.rm = TRUE,
             metrics = c("twistc")) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("TwistcSubset", i, ".tif"), overwrite = TRUE)

rm(Twistc)
gc()

print("Calculating BPI")
BPI = BPI(rastSubset,
             w = c(9,9),
             na.rm = TRUE) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("BPISubset", i, ".tif"), overwrite = TRUE)

rm(BPI)
gc()

print("Calculating Adjusted SD")
AdjSD = AdjSD(rastSubset,
             w = c(9,9),
             na.rm = TRUE) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("AdjSDSubset", i, ".tif"), overwrite = TRUE)

rm(AdjSD)
gc()

print("Calculating VRM")
VRM = VRM(rastSubset,
             w = c(9,9),
             na.rm = TRUE) %>% 
  mask(subsetBuff2) %>% crop(subsetBuff2, filename = str_c("VRMSubset", i, ".tif"), overwrite = TRUE)

rm(VRM)
gc()

}

#Take all the subsets for each metric calculated and merge them into one.
metrics = c("Slope", "Northness", "Eastness", "Profc", "Planc", "Twistc", "BPI", "AdjSD", "VRM")

setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")

for (i in 1:length(metrics)) {
  
  #Import all the rasters from a given metric and store as sprc. Then merge them back to one large raster. Becuase we use such large overlaps in our buffers the overlaps present between the rasters here contain identical values.
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")
  stack = list.files() %>% 
    str_subset(pattern = str_c(metrics[i], "Subset")) %>% 
    sprc() %>% 
    merge(filename = str_c(metrics[i], "Statewide.tif"), 
            overwrite = TRUE)
  
}


#Also aggregate the terrain metrics to 50m res so they match the bathy.
for (i in 1:length(metrics)) {
  
  #Import all the rasters from a given metric and then export to 50m res using the mean cell value.
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")
  raster = list.files() %>% 
    str_subset(pattern = str_c(metrics[i], "Statewide.tif")) %>% 
    rast() %>% 
    aggregate(fact = 10,
              fun = "mean",
              na.rm = TRUE,
              filename = str_c(metrics[i], "Statewide50mMean.tif"), 
              overwrite = TRUE)
  
    raster = list.files() %>% 
    str_subset(pattern = str_c(metrics[i], "Statewide.tif")) %>% 
    rast() %>% 
    aggregate(fact = 10,
              fun = "sd",
              na.rm = TRUE,
              filename = str_c(metrics[i], "Statewide50mSD.tif"), 
              overwrite = TRUE)
}


```

### 2) Wave data

Next off, import Wave data from Jin Liu (DEECA). These are models of the wave climate of victoria from https://doi.org/10.1016/j.renene.2023.118943 and https://doi.org/10.1016/j.ocemod.2022.101980. We will import all available data for the years 1992 through 2023, calculate the daily average (mean) and then compute an annual average and SD and a winter average and 95% CI. This will be completed for significant wave height, partitioned wind wave, partitioned primary and secondary swells, peak wave frequency, mean wave freq (1/mean wave period). The idea behind using averages and seasonal measures is to capture the general conditions species present are exposed to and to also capture the season that is likely to be most stressful for them (in this case, winter).

Wave variables and their meanings:
Signif wave height = average height of the largest 1/3 of the waves. E.g. How big are the biggest waves?
Peak Wave Freq = frequency (Hz) of the highest energy waves. E.g. how common are the big waves?
Mean Wave Frequency = average freq of all waves in the unit time.
Wind Wave = swell of waves originating from local wind forces.
Primary and secondary swell heights = Primary swells occurs from far offshore and are normally larger, longer periods, and higher amplitudes. Secondaries are smaller, weaker.

Data are in the form of one netcdf for each variable and year combination. So lots of looping is required here. Data available only span 1981 to 2020 so we will have to upper bound our analyses at 2020. We can still lower bound at 1992

Note. As of 13.6.2024 meeting we now calculate the monthly mean not the seasonal mean. We also dropped the 95th as a measure of interest for temperature as it was taking too long to code.

```{r, eval = FALSE}
#Prep work
directoriesList = c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/SignifWaveHeight") #Only working with signif wave height right now
                    #"Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/PeakWaveFreq",
                    #"Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/MeanWavePeriod",
                    #"Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/PartitionedWindWave",
                    #"Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/PartitionedPrimarySwell",
                    #"Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/PartitionedSecondarySwell")

years = seq(1992, 2020, 1) %>% as.character()

metrics = c("hs") #, "fp", "t02", "phs0", "phs1", "phs2") #Currently only using signif wave height due to masive times needed to interpolate data

#Start outer loop here. Controls the variable we are working with.
for (j in 1:length(directoriesList)){

print(str_c("Currently working on", " ", metrics[j]))

files = map(years, str_subset, string = directoriesList[j] %>% list.files()) %>% reduce(union) #Map function to the rescue!

#Connect to first netcdf and extract matrix dimensions for Lat Lon.
data = nc_open(str_c(directoriesList[j], "/", files[1])) #Only get first slice of data

#Make holder arrays. Array as above but with N layers where N = number of years we're working with. Need one per variable we want to extract.
lat = ncvar_get(data, "latitude") #Get node lat
lon = ncvar_get(data, "longitude") #Get node lon
annualMean = matrix(data = NA, nrow = length(lat), ncol = length(years)) #Rows = number of nodes, Cols = N years assessed
annualSD = matrix(data = NA, nrow = length(lat), ncol = length(years)) #Rows = number of nodes, Cols = N years assessed
monthlyMean = matrix(data = NA, nrow = length(lat), ncol = length(years)*12) #Rows = number of nodes, Cols = N years assessed*12 months
monthlySD = matrix(data = NA, nrow = length(lat), ncol = length(years)*12) #Rows = number of nodes, Cols = N years assessed*12 months

#Close connection from initial netcdf4 access
nc_close(data)

#From here on the values change based on the file we read in so we need to change them iteratively over loops. Inner loop.
for (i in 1:length(files)) {
  
print(str_c("Completing loop", " ", i))

data = nc_open(str_c(directoriesList[j], "/", files[i]))

date = ncvar_get(data, "time") %>% as.Date(origin = "1990-01-01 00:00:00", tz = "Australia/Victoria") %>% as.character() #Grab date of each time slice. 24 slices per day (so each slice is an hour).

fullArray = ncvar_get(data, metrics[j]) #Data are in long form. 97276 nodes (not in lat long - rows) and time (number of time entries varies between years - cols).

dailyMean = fullArray  %>% t() %>% data.frame() #Transpose so days are now rows

dailyMean = dailyMean %>% 
  mutate(date = date) %>% #Add grouping variable that is date
  fgroup_by(date) %>% #Using f functions as they're super fast versions of the normal dplyr stuff
  fmean(na.rm = TRUE) %>% #Calc daily mean
  mutate(month = month(date)) #Append new month column

#We can now calculate our annual measures (mean and SD). Returns vectors of values which can be slotted straight into holders.
annualMean[,i] = dailyMean %>% 
  dplyr::select(!c(date, month)) %>% 
  fmean(na.rm = TRUE)

annualSD[,i] = dailyMean %>% 
  dplyr::select(!c(date, month)) %>% 
  fsd(na.rm = TRUE)

#And our seasonal measures - wave data want monthly mean, SD, 95th. Returns vectors of values which can be slotted straight into holders.
monthlyMean[,((i*12)-11):(i*12)] = dailyMean %>% 
  dplyr::select(!date) %>%
  fgroup_by(month) %>% 
  fmean() %>%
  dplyr::select(!month) %>% 
  t()

monthlySD[,((i*12)-11):(i*12)] = dailyMean %>% 
  dplyr::select(!date) %>%
  fgroup_by(month) %>% 
  fsd() %>% 
  dplyr::select(!month) %>% 
  t()

#Drop objects to save RAM, disconnect from netcdf, then close loop after final iteration
nc_close(nc = data)
rm(date, fullArray, dailyMean, data)

}

#Now the variable is done we can export the matrices as separate R files to save RAM.
saveRDS(annualMean,
        str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
             directoriesList[j] %>% str_split(pattern = "/") %>% unlist() %>% dplyr::last(),
             "AnnualMean"))

saveRDS(annualSD,
        str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
             directoriesList[j] %>% str_split(pattern = "/") %>% unlist() %>% dplyr::last(),
             "AnnualSD"))

saveRDS(monthlyMean,
        str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
             directoriesList[j] %>% str_split(pattern = "/") %>% unlist() %>% dplyr::last(),
             "MonthlyMean"))

saveRDS(monthlySD,
        str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
             directoriesList[j] %>% str_split(pattern = "/") %>% unlist() %>% dplyr::last(),
             "MonthlySD"))

#Drop objects we'll remake at the start
rm(annualMean, annualSD, monthlyMean, monthlySD)

#Close outer loop
}

```

We have the frequency for the peak wave data but the period for the mean wave data. Let's do a quick loop and remedy that and convert the mean wave period to the mean wave freq.
Currently not used as we dont include wave period or frequency in our modelling due to time constraints with interpolations.

```{r, eval = FALSE}
#files = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves") %>% 
#  str_subset(pattern = "Period")

#for (i in 1:length(files)){
  
#  data = readRDS(str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
#             files[i]))
  
#  data = 1/data #Convert period to frequency
  
#  saveRDS(data,
#          str_c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/",
#                files[i]) %>%
#            str_replace(pattern = "Period", replacement = "Freq"))
#}

```

Now, let's convert our datasets into XYZ format so they're easier to work with in a spatial setting.

```{r, eval = FALSE}
#Grab Lat and Long data from original netcdf. These are in a worldwide projection EPSG4326.
data = nc_open("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/VICCoastWW3RawOutputs/Contemporary1981To2020/MeanWaveDirection/ww3.hindcast_1981_dir.nc") #Just an example nc, they all have identical dimensions
latitude = ncvar_get(data, "latitude") #%>% data.frame() %>% rename(latitude = names(.)[1])
longitude = ncvar_get(data, "longitude") #%>% data.frame() %>% rename(longitude = names(.)[1])
year = seq(1992, 2020, 1) %>% as.character() #Define years we know are in the data
yearmonth = seq(as.Date("1992-01-01", tz = "Australia/Victoria"), as.Date("2020-12-31", tz = "Australia/Victoria"), by = "month") %>% format("%Y-%m") #Define years and months
nc_close(data)

#Convert the location data to an sf object so we can get spatial. 
#MAJOR NOTE!!! These data are already in column form (i.e. we dont need to expand the grid as we do the temp data). We can just bind them as columns.
XYsf = data.frame(longitude = longitude,
                  latitude = latitude) %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% 
  mutate(longitude = st_coordinates(.)[,1],
         latitude = st_coordinates(.)[,2]) %>% 
  st_drop_geometry()

#Import the data we wish to interpolate.
directoriesList = "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves/"
files = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves") %>% str_subset(pattern = "XYZ", negate = TRUE)

#Loop through each layer of the array and add the years data as a separate column
for (j in 1:length(files)) {
  
  print(str_c("Working on variable ", files[j]))

  #Read in array values
  arrayValues = readRDS(str_c(directoriesList[1],
    files[j]))
  
  #Make a copy to muck around with in the inner loop
  Holder = XYsf

  for (i in (1:dim(arrayValues)[2])){
  
    print(str_c("Processing slice ", i))
  
    Holder[[i+2]] = arrayValues[,i]
  }

  #Rename Columns
  if(str_detect(files[j], pattern = "Monthly")){
    print("Monthly Data Detected... renaming cols")
    names(Holder)[3:length(names(Holder))] = unique(yearmonth)
  } else {
    print("Annual Data Detected...renaming cols")
    names(Holder)[3:length(names(Holder))] = unique(year)[3:ncol(Holder)-2]}
  
  #Save
  saveRDS(Holder,
          str_c(directoriesList[1],
                files[j],
                "XYZ"))
}



```

### 3) Temperature Data

Now we've got the wave predictors wrangled and exported (but notably not interpolated quite yet) we can rinse and repeat for the temperature data. This data is sourced from the AODN (Australian Ocean Data Network). Data are monthly means collated from daily data. Data are NOAA Daily Optimum Interpolation Sea Surface Temperature originally but aggregated to monthly means. Relevant papers are Reynolds, et al.(2007) https://doi.org/10.1175/2007JCLI1824.1), Banzon, et al.(2016) https://doi.org/10.5194/essd-8-165-2016), Huang, B., C. Liu, V. Banzon, E. Freeman, G. Graham, B. Hankins, T. Smith, and H.-M. Zhang, 2021 https://doi.org/10.1175/JCLI-D-20-0166.1). Data links: 

Description.....https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00680.
Download......https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html

Shout out to Dr Alex Shute for handling the data export for these variables.

Generally we can take the same steps as we did above for each of the wave variables. But, given data are already monthly means we can save some coding. We want the mean and SD annually and the monthly mean. Given the data are already monthly means we cant get an associated SD. As with wave data, this change was implemented as of the 13.6.2024 meeting.

```{r, eval = FALSE}
#Investigate the netcdfs first
directoriesList = c("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Temperature")

files = list.files(directoriesList) %>% str_subset(pattern = ".nc")#Map function to the rescue!

#First dataset 
data = nc_open(str_c(directoriesList, "/", files[2]))

data

date = ncvar_get(data, "time") %>% as.Date(origin = "1800-01-01 00:00:00", tz = "Australia/Victoria") %>% as.character()

print(date[1]) #Time starts at 1981-09-01
print(date[length(date)-1]) #And ends at 2024-03-01

rm(date) #Don't need this level of detail as we are working with annual means and monthly data

yearmonth = ncvar_get(data, "time") %>% as.Date(origin = "1800-01-01 00:00:00", tz = "Australia/Victoria") %>% format("%Y-%m") %>% as.character()
year = ncvar_get(data, "time") %>% as.Date(origin = "1800-01-01 00:00:00", tz = "Australia/Victoria") %>% format("%Y") %>% as.character()
month = ncvar_get(data, "time") %>% as.Date(origin = "1800-01-01 00:00:00", tz = "Australia/Victoria") %>% format("%m") %>% as.character()

#Load in full array of all data
fullArray = ncvar_get(data, "sst")
dim(fullArray) #Long, Lat, Time

#Calculate Annual Mean and SD

AnnualMeanHolder = array(data = NA, dim = c(108, 80, year %>% unique() %>% length()))
AnnualSDHolder = array(data = NA, dim = c(108, 80, year %>% unique() %>% length()))

for (i in 1:length(unique(year))){
  
  print(str_c("Calculting year ", i))
  
  subset = fullArray[, , min(which(year == unique(year)[i])) : max(which(year == unique(year)[i]))] #Load ith year of data in
  
  AnnualMeanHolder[, , i] = apply(subset, MARGIN = c(1,2), FUN = "mean", na.rm = TRUE) #Calc mean and store in ith layer
  AnnualSDHolder[, , i] = apply(subset, MARGIN = c(1,2), FUN = "sd", na.rm = TRUE) #Calc se and store in ith layer
  
}

#Export Annual Measures
saveRDS(AnnualMeanHolder,
        "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/TemperatureAnnualMean")

saveRDS(AnnualSDHolder,
        "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/TemperatureAnnualSD")


#Repeat for monthly equivalents. Just the original netcdf data.
monthlyMeanHolder = fullArray

#Export Annual Measures
saveRDS(monthlyMeanHolder,
        "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/TemperatureMonthlyMean")


```

Like we did for the wave data, let's now change our data into an XYZ format. This will make things easier for our interpolations.

```{r, eval = FALSE}
#Grab Lat and Long data from original netcdf. These are in a worldwide projection EPSG4326.
data = nc_open("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Temperature/noaa_sst_masked_southeast_aus.nc")
latitude = data$dim$lat$vals #%>% data.frame() %>% rename(latitude = names(.)[1])
longitude = data$dim$lon$vals #%>% data.frame() %>% rename(longitude = names(.)[1])
yearmonth = ncvar_get(data, "time") %>% as.Date(origin = "1800-01-01 00:00:00", tz = "Australia/Victoria") %>% format("%Y-%m") %>% as.character()
year = ncvar_get(data, "time") %>% as.Date(origin = "1800-01-01 00:00:00", tz = "Australia/Victoria") %>% format("%Y")
nc_close(data)

#Convert the location data to an sf object so we can get spatial.
XYsf = base::expand.grid(longitude, latitude) %>% 
  rename(latitude = names(.)[2],
         longitude = names(.)[1]) %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% 
  mutate(longitude = st_coordinates(.)[,1],
         latitude = st_coordinates(.)[,2]) %>% 
  st_drop_geometry()

#Import the data we wish to interpolate.
directoriesList = "Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/"
files = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature") %>% str_subset(pattern = "XYZ", negate = TRUE)

#Loop through each layer of the array and add the years data as a separate column
for (j in 1:length(files)) {
  
  print(str_c("Working on variable ", files[j]))

  #Read in array values
  arrayValues = readRDS(str_c(directoriesList[1],
    files[j]))
  
  #Make a copy to muck around with in the inner loop
  Holder = XYsf

  for (i in (1:dim(arrayValues)[3])){
  
    print(str_c("Processing slice ", i))
  
    Holder[[i+2]] = arrayValues[,,i] %>% matrix(ncol = 1, byrow = FALSE)
  }

  #Rename Columns
  if(str_detect(files[j], pattern = "Monthly")){
    print("Monthly Data Detected... renaming cols")
    names(Holder)[3:length(names(Holder))] = yearmonth
  } else {
    print("Annual Data Detected...renaming cols")
    names(Holder)[3:length(names(Holder))] = unique(year)[3:ncol(Holder)-2]}
  
  #Save
  saveRDS(Holder,
          str_c(directoriesList[1],
                files[j],
                "XYZ"))
}

```


### 4) Response Data Wrangling.

Before we can interpolate these data we need to know where our response data are, both in space and in time. There is no point interpolating data for years we dont need as this will just burn time.

First we need to get the response data organised into a format HMSC will take.

```{r}
#Import data and bin off any rows where the RSL gang didnt see a single species of the focal taxanomic group
fishRaw = read.csv("Z:/GIS-Res/Projects/ParksVic_Hmsc/ResponseData/Unwrangled/ep_m1_fish_victoria.csv") %>% 
    filter(!str_detect(recorded_species_name, pattern = "No species found")) 
  
invertRaw = read.csv("Z:/GIS-Res/Projects/ParksVic_Hmsc/ResponseData/Unwrangled/ep_m2_inverts_victoria.csv") %>% 
    filter(!str_detect(recorded_species_name, pattern = "No species found")) 

#Structure of both
str(fishRaw)
str(invertRaw)

#Wrangle. Sum of each species in each transect, in each site, during each year, pivoted so cols are on the vertical.
fishLatLon = fishRaw %>% mutate(year = as.Date(survey_date) %>% format("%Y")) %>% 
  dplyr::select(site_code, site_name, location, latitude, longitude, year, survey_date) %>% 
  group_by(site_name, site_code, year, survey_date, location) %>% 
  summarise(latitude = mean(latitude, na.rm = TRUE),
            longitude = mean(longitude, na.rm = TRUE)) %>% 
  ungroup()

#Check each site has a unique lat long. Suggests two sites have different site IDs but identical lat longs.
temp = fishLatLon %>% 
  group_by(site_code) %>% 
  summarise(longitude = mean(longitude, na.rn = TRUE),
            latitude = mean(latitude, na.rm = TRUE)) %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326")

length(unique(temp$site_code)) #188
length(unique(temp$geometry)) #187

#Find duplicated geom
distinct = dplyr::distinct(temp, geometry, .keep_all = TRUE)
dupedGeom = temp[!(temp$site_code %in% distinct$site_code),]

#Find all rows with this geom
identified = temp[temp$geometry == dupedGeom$geometry,] #PPB25 and PPB8 have identical lat longs. Looking at the data these are called St Leonards and St Leonards rock wall. 

#Repeat for inverts
#Wrangle. Sum of each species in each transect, in each site, during each year, pivoted so cols are on the vertical.
invertLatLon = invertRaw %>% mutate(year = as.Date(survey_date) %>% format("%Y")) %>% 
  dplyr::select(site_code, site_name, location, latitude, longitude, year, survey_date) %>% 
  group_by(site_name, site_code, year, survey_date, location) %>% 
  summarise(latitude = mean(latitude, na.rm = TRUE),
            longitude = mean(longitude, na.rm = TRUE)) %>% 
  ungroup()

#Check each site has a unique lat long. Suggests two sites have different site IDs but identical lat longs.
temp = invertLatLon %>% 
  group_by(site_code) %>% 
  summarise(longitude = mean(longitude, na.rn = TRUE),
            latitude = mean(latitude, na.rm = TRUE)) %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326")

length(unique(temp$site_code)) #187
length(unique(temp$geometry)) #186

#Find duplicated geom
distinct = dplyr::distinct(temp, geometry, .keep_all = TRUE)
dupedGeom = temp[!(temp$site_code %in% distinct$site_code),]

#Find all rows with this geom
identified = temp[temp$geometry == dupedGeom$geometry,] #PPB25 and PPB8 have identical geoms. Same sites as above.

#Now compute the occurrence matrix for fish then inverts

fishCounts = fishRaw %>% mutate(year = as.Date(survey_date) %>% format("%Y")) %>% 
  dplyr::select(site_code, site_name, location, year, survey_date, species_name, total) %>% 
  group_by(site_name, site_code, survey_date, year, location, species_name) %>% 
  summarise(count = sum(total, na.rm = TRUE)) %>% 
  pivot_wider(id_cols = c("site_name", "site_code", "survey_date", "location", "year"),
              names_from = "species_name",
              values_from = "count",
              values_fill = 0) %>% 
  dplyr::filter(!site_code == "PPB8") #Drop suspicious st leonards data

invertCounts = invertRaw %>% mutate(year = as.Date(survey_date) %>% format("%Y")) %>% 
  dplyr::select(site_code, site_name, location, year, survey_date, species_name, total) %>% 
  group_by(site_name, site_code, survey_date, year, location, species_name) %>% 
  summarise(count = sum(total, na.rm = TRUE)) %>% 
  pivot_wider(id_cols = c("site_name", "site_code", "survey_date", "location", "year"),
              names_from = "species_name",
              values_from = "count",
              values_fill = 0) %>% 
  dplyr::filter(!site_code == "PPB8") #Drop suspicious st leonards data


#Combine datasets and export now wrangled
fishMaster = full_join(fishLatLon, fishCounts) %>% na.omit() #Drop pesky PPB8 site as the lat long was still in the first dataset joined
invertMaster = full_join(invertLatLon, invertCounts) %>% na.omit()

write.csv(fishMaster,
          "Z:/GIS-Res/Projects/ParksVic_Hmsc/ResponseData/Wrangled/fishMaster.csv")

write.csv(invertMaster,
          "Z:/GIS-Res/Projects/ParksVic_Hmsc/ResponseData/Wrangled/invertMaster.csv")

```


### 5) Interpolations

Now we need to interpolate the results of the temperature and wave data arrays to the template raster. 

First off, not all the years we have predictor data for are actually in our response data. Let's import the response data and muck around with it.

```{r}
#Import fish and invert response data
fishData = read.csv("Z:/GIS-Res/Projects/ParksVic_Hmsc/ResponseData/Wrangled/fishMaster.csv")
invertData = read.csv("Z:/GIS-Res/Projects/ParksVic_Hmsc/ResponseData/Wrangled/invertMaster.csv")

#Some of our datapoints are outside of our focal polygon. Drop em.
coastlinePoly = st_read("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VIC_coastalwaters_mask.shp") %>% st_transform(crs = "EPSG:7899")

fishDataSf = fishData %>% st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% st_transform(crs = "EPSG:7899") %>% st_intersection(coastlinePoly) %>% 
  dplyr::select(-c("FID_cw_sta", "FEATURE", "REL_DATE", "FID_vic25_", "TYPE2", "COUNT", "FIRST_FEAT", "Area"))

invertDataSf = invertData %>% st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% st_transform(crs = "EPSG:7899") %>% st_intersection(coastlinePoly) %>% 
  dplyr::select(-c("FID_cw_sta", "FEATURE", "REL_DATE", "FID_vic25_", "TYPE2", "COUNT", "FIRST_FEAT", "Area"))

#How many rows did we drop from each dataset?
print(nrow(fishData) - nrow(fishDataSf)) #36
print(nrow(invertData) - nrow(invertDataSf)) #35

#Years present in each dataset
setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/ResponseData/Wrangled/")
fishYears = fishDataSf$survey_date %>% as.Date() %>% format("%Y") %>% unique()
fishYears %>% saveRDS("fishYears")
invertYears = fishDataSf$survey_date %>% as.Date() %>% format("%Y") %>% unique()
invertYears %>% saveRDS("invertYears")

#Do fish years match the invert years?
fishYears == invertYears #Yep all match

```

Beginning interpolations using IDW. We have a gridded dataset for each variable (either from the satellite derived temp or from a fine scale variable grid for the waves). So, the IDW algorithm is a tolerable choice. In later parts of the project we might use INLA.

Begin with temperature.

```{r}
#Import the bathymetry layer we are using as our template for xys we want to interpolate onto
templateRast = rast("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VICBathy50mMean.tif")

#Make polygon version of the bathymetry
templatePoly = templateRast %>% ext() %>% as.polygons(crs = "EPSG:7899") %>% st_as_sf()

#Remember!!! The temperature data begin in WGS84 as they come from a global dataset.
variableList = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature") %>% 
  str_subset(pattern = "XYZ", negate = TRUE) #Keep only those without. Five temperature measures to work through.

#Define focal years. From a meeting held on 13.6.2024 we know we only want to target 2002-2020. No point interpolating years we wont need.
focalYears = seq(2002, 2020, 1) %>% as.character()

for (j in 1:length(variableList)){
  
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature")

  filesList = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature") %>% 
                          str_subset(string = ., pattern = "XYZ", negate = FALSE)
  
  print(str_c("Working on ", filesList[j]))
  
  ToInterpolate = readRDS(filesList[j]) %>%  #Read in jth file
  st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% 
    st_transform(crs = "EPSG:7899")
  
  focalCols = c(focalYears, "geometry")
  
  ToInterpolate = ToInterpolate[,str_detect(names(ToInterpolate), pattern = paste(focalCols, collapse = "|"))] #Drop cols we dont want to keep
  
for (i in 1:(ncol(ToInterpolate)-1)){ #Skip last col as it's geometry
 
  print(str_c("Interpolating ", names(ToInterpolate)[i], " Data"))
  
  temp = sf_to_rast(observer = ToInterpolate[,i] %>% na.omit(),
                    v = names(ToInterpolate)[i],
                    aoi = templatePoly,
                    beta = 2,
                    raster_res = 50,
                    cores = 20,
                    progress = TRUE) #Compute IDW. All points allowed to inform prediction of all other points. Temp data so coarse and effective spatial range so large that this is sensible. Using 20 cores to make things as fast as possible. Machine has 24.
  
  crs(temp) = crs(templateRast) #Reassign crs
  
  names(temp) = str_c(filesList[j] %>% str_replace(pattern = "XYZ", replacement = ""),
                      names(ToInterpolate)[i])
  
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Rasters")

  temp = temp %>% mask(templateRast,
                       filename = str_c(names(temp),"Rast.tif"),
                       overwrite = TRUE)
  rm(temp)
   
}
  
}

```

Repeat for wave data. Takes multiple days!

```{r}
#Import the bathymetry layer we are using as our template for xys we want to interpolate onto
templateRast = rast("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VICBathy50mMean.tif")

#Make polygon version of the bathymetry
templatePoly = templateRast %>% ext() %>% as.polygons(crs = "EPSG:7899") %>% st_as_sf()

#Make another one of just the area we are focussing in. Not it's extent.
templatePolyCoastal = templateRast %>% as.polygons(crs = "EPSG:7899") %>% st_as_sf()

#Remember!!! The data begin in WGS84 as they come from a global dataset.
variableList = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves") %>% 
  str_subset(pattern = "XYZ", negate = TRUE) %>% 
  str_subset(string = ., pattern = "Signif", negate = FALSE) #Keep only those without. 35 wave measures to work through. But see note a few lines down.

###NOTE - DUE TO THE CONSIDERABLE TIME TAKEN TO INTERPOLATE THESE DATA AND THE TIGHT TURN AROUNDS FOR THE PROJECT WE ONLY EXTRACT SIGNIF WAVE HEIGHT AS A FOCAL VARIABLE RIGHT NOW ###

#Define focal years. From a meeting held on 13.6.2024 we know we only want to target 2002-2020. No point interpolating years we wont need.
focalYears = seq(2002, 2020, 1) %>% as.character()

#There are lots of wave data so we need to trim the data we import down to prevent computer crashing when we interpolate. Doing this outside the loops.
#Makes a list of rows that are NA where the point falls outside of the polygon.
setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves")

filesList = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves") %>% 
                          str_subset(string = ., pattern = "XYZ", negate = FALSE)

InsideCoastline = readRDS(filesList[1]) %>% dplyr::select(c(longitude, latitude, any_of(contains(focalYears)))) %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% 
    st_transform(crs = "EPSG:7899") %>% 
    vect() %>% 
    terra::extract(y = ., x = templateRast, ID = FALSE)

for (j in 1:length(variableList)){
  
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves")

  filesList = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Waves") %>% 
                          str_subset(string = ., pattern = "XYZ", negate = FALSE) %>% 
                          str_subset(string = ., pattern = "Signif", negate = FALSE) 
  
  print(str_c("Working on ", filesList[j]))
  
  ToInterpolate = readRDS(filesList[j]) %>%  #Read in jth file
  st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% 
    st_transform(crs = "EPSG:7899")
  
  focalCols = c(focalYears, "geometry")
  
  ToInterpolate = ToInterpolate[,str_detect(names(ToInterpolate), pattern = paste(focalCols, collapse = "|"))] #Drop cols we dont want to keep
  
  ToInterpolate = ToInterpolate[!is.na(InsideCoastline),] #Drop rows outside of focal polygon.
  
  #Point count is still super high for the time we have to interpolate. Let's subset with a spatially balanced approach to 25%.
  set.seed(1995) #Standardise subsampling randomness
  N = nrow(ToInterpolate)
  n = N/4
  p = rep(n/N,N) #Uniform inclprobs
  x = matrix(runif(N * 2), ncol = 2);
  s = cube(p, x)
  
  ToInterpolate = ToInterpolate[s,] #Select subsetted rows
  
  names(ToInterpolate) = names(ToInterpolate) %>% str_replace(pattern = "X", replacement = "") #SF introduced X's to variable names (1998 becomes X1998) because technically a numerical col name is hard to work with. Change this back to what we want.
  
for (i in 1:(ncol(ToInterpolate)-1)){ #Skip last col as it's geometry
 
  print(str_c("Interpolating ", names(ToInterpolate)[i], " Data"))
  
  temp = sf_to_rast(observer = ToInterpolate[,i] %>% na.omit(),
                    v = names(ToInterpolate)[i],
                    aoi = templatePoly,
                    n = 12,
                    beta = 2,
                    raster_res = 50,
                    cores = 20,
                    progress = TRUE) #Compute IDW. Only the nearest 12 neighbours are used to calculate the prediction of a given cell. Stop all 20-30k points from being used to derive one cell value. Let R get at 20 cores to speed things up.
  
  crs(temp) = crs(templateRast) #Reassign crs
  
  names(temp) = str_c(filesList[j] %>% str_replace(pattern = "XYZ", replacement = ""),
                      names(ToInterpolate)[i])
  
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Rasters")

  temp = temp %>% mask(templateRast,
                       filename = str_c(names(temp),"Rast.tif"),
                       overwrite = TRUE)
  rm(temp)
   
}
  
}


```

### 6) Form modelling dataset

Now that we have the predictor data interpolated we can begin to form our dataset for model fitting. We want to get the following data in there as character classes that can eventually become factors.

- Whether the point is within an MPA or is an associated reference site
- What MPA site is the data point associated with (we have reference data on the edge of some MPAs that are used for)
- An indicator of the geographic region the point is in
- IMCRA bioregions

First, we need raster data for all of these.

```{r}
#Add in IMCRA bioregion - polygon derived from one that is publically available, needs a bit of mucking around with to match our template raster.
templateRast = rast("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VICBathy50mMean.tif")

setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/EcologicalClassifications")
imcraRast =  st_read("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/EcologicalClassifications/Integrated_Marine_and_Coastal_Regionalisation_of_Australia_(IMCRA)_v4.0_-_Provincial_Bioregions.shp") %>% 
  st_transform(crs = "EPSG:7899") %>%
  vect() %>% 
  terra::rasterize(templateRast, field = "PB_NAME") #Only gives cell values where the polygons were

tempRast = focal(imcraRast, w = 9, fun = "modal", na.policy = "only") #Window size corresponds to ~450m. Far enough to make sure we get data for where the polygon ends abruptly but not huge enough to give wonky values. 
tempRast[is.na(templateRast)] = NA #Any NAs in template rast remain NAs here
levels(tempRast) = terra::levels(imcraRast) #Re-assign levels
imcraRast = tempRast #Swap objects back over
names(imcraRast) = "Bioregion"
rm(tempRast)
writeRaster(imcraRast, "imcraRast.tif", overwrite = TRUE) #Export

#Add in whether the name of the MPA the point lands in, if it does so. MPA layer is also crude around edges as it's based on the IMCRA polygon. Same approach applied.
######### This is something we need to clear up in the meeting with parks... how to we predict named regions in the future that arent MPA boundaries? #############
setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/EcologicalClassifications")
mpaStatusRast = st_read("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/EcologicalClassifications/VEAC_CBiCS_classification_VIC.shp") %>% 
  st_transform(crs = "EPSG:7899") %>%  
  vect() %>% 
  terra::rasterize(templateRast, field = "MPA_status") #Only gives cell values where the polygons were

tempRast = focal(mpaStatusRast,  w = 9, fun = "modal", na.policy = "only") #Window size corresponds to ~450m. Far enough to make sure we get data for where the polygon ends abruptly but not huge enough to give wonky values. 
tempRast[is.na(templateRast)] = NA #Any NAs in template rast remain NAs here
levels(tempRast) = terra::levels(mpaStatusRast) #Re-assign levels
mpaStatusRast = tempRast #Swap objects back over
names(mpaStatusRast) = "MPAName"
rm(tempRast)
writeRaster(mpaStatusRast, "mpaRast.tif", overwrite = TRUE) #Export

#Repeat but this time we get the broad region of each polygon (Level 6 Cbics specifically). These are essentially a polygon of the zone the data belong to (PPB, Cape Conran, etc).
setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/EcologicalClassifications")
regionRast = st_read("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/EcologicalClassifications/VEAC_CBiCS_classification_VIC.shp") %>% 
  st_transform(crs = "EPSG:7899") %>%  
  vect() %>% 
  terra::rasterize(templateRast, field = "Level_6") #Only gives cell values where the polygons were

tempRast = focal(regionRast,  w = 9, fun = "modal", na.policy = "only") #Window size corresponds to ~450m. Far enough to make sure we get data for where the polygon ends abruptly but not huge enough to give wonky values. 
tempRast[is.na(templateRast)] = NA #Any NAs in template rast remain NAs here
levels(tempRast) = terra::levels(regionRast) #Re-assign levels
regionRast = tempRast #Swap objects back over
names(regionRast) = "Region"
rm(tempRast)
writeRaster(regionRast, "regionRast.tif", overwrite = TRUE) #Export

#Finally we can muck around with another class denoting whether a site is protected or not based on whether a named MPA was found earlier.
#Make raster predictor and export
protectRast = mpaStatusRast
protectRast[!is.na(protectRast) & protectRast == 16] = 0 #0 = unprotected
protectRast[!is.na(protectRast) & protectRast != 0] = 1 #1 = protected
levels(protectRast) = data.frame(ID = c(0,1),
                                 ProtectionStatus = c("Unprotected", "Protected"))

names(protectRast) = "MPAProtectStat"

setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/EcologicalClassifications")
writeRaster(protectRast, "mpaProtectionRast.tif", overwrite = TRUE) #Export



```

Now we can make stacked rasters which correspond to each year of data. These will be very useful for future modelling and prediction.

We can then ammend the data to the associated year-specifc rows in the fish and invert datasets.

```{r}
#Here we loop again, extracting a given year of either fish or invert data, finding the temperature and wave data from that year, then extracting using terra, finally we recompile the data.

#Import fish and invert response data
fishData = read.csv("Z:/GIS-Res/Projects/ParksVic_Hmsc/ResponseData/Wrangled/fishMaster.csv")
invertData = read.csv("Z:/GIS-Res/Projects/ParksVic_Hmsc/ResponseData/Wrangled/invertMaster.csv")

#Some of our datapoints are outside of our focal polygon. Drop em.
coastlinePoly = st_read("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VIC_coastalwaters_mask.shp") %>% st_transform(crs = "EPSG:7899")

fishDataSf = fishData %>% st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% st_transform(crs = "EPSG:7899") %>% st_intersection(coastlinePoly) %>% 
  dplyr::select(-c("FID_cw_sta", "FEATURE", "REL_DATE", "FID_vic25_", "TYPE2", "COUNT", "FIRST_FEAT", "Area"))

invertDataSf = invertData %>% st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% st_transform(crs = "EPSG:7899") %>% st_intersection(coastlinePoly) %>% 
  dplyr::select(-c("FID_cw_sta", "FEATURE", "REL_DATE", "FID_vic25_", "TYPE2", "COUNT", "FIRST_FEAT", "Area"))

#How many rows did we drop from each dataset?
print(nrow(fishData) - nrow(fishDataSf))
print(nrow(invertData) - nrow(invertDataSf))

#Note we only have wave data up until 2020 so lets drop any fish/invert data beyond this. We also only want data starting from 2002 as this is when the focal MPAs were established.
fishDataSf = fishDataSf %>% filter(year %in% seq(2002, 2020))

#And repeat for invert data
invertDataSf = invertDataSf %>% filter(year %in% seq(2002, 2020))


#Now we can begin extracting the data - year only first
years = fishDataSf$year %>% unique() %>% sort() #Get years to focus on in ascending order. Previously we have demonstrated that the years between fish and inverts are identical.
fishlist = list() #Holder for our fish dfs
invertlist = list() #Holder for our invert dfs

for (i in years){
  
  print(paste("Working on", i))
  
    #Import year i rasters
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Rasters")
  stack1 = rast(
    list.files() %>% str_subset(pattern = as.character(i)) %>% str_subset(pattern = "Annual")) #Get wave and temp rasters. But only the year-specific ones. Not the monthly ones.
  
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/EcologicalClassifications")
  stack2 = rast(
    list.files() %>% str_subset(pattern = "Rast") %>% #Get MPA, protection status, region, and imcra rasters
    str_subset(pattern = ".aux.xml", negate = TRUE))
    
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry")
  stack3 = rast(
    list.files() %>% str_subset(pattern = "50mMean") %>% #Get statewide terrain metrics and bathy
    str_subset(pattern = ".aux.xml", negate = TRUE))
    
  stack = c(stack1, stack2, stack3)
  names(stack)[17] = "Depth" #Rename to something simpler
  names(stack) = names(stack) %>% str_replace(pattern = i %>% as.character(), replacement = "")
  
  stack = mask(stack, stack) #If one layer has an NA in a given cell locale make it NA in all layers
  
  rm(stack1, stack2, stack3)
  
  #Export year stack for later use
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/Modelling/PredictorStacks")
  writeRaster(stack, str_c("PredictorStack", i, ".tif"), overwrite = TRUE)
   
  #Append to relevant year of fish data. i-2001 converts our active year to 1, 2, 3 etc  
  fishlist[[i-2001]] = fishDataSf %>% filter(year == i) %>% 
    vect() %>% 
    terra::extract(y = .,
                   x = stack,
                   bind = TRUE) %>% 
    st_as_sf() %>% 
    st_transform(crs = "EPSG:4326") %>% 
    mutate(longitude = st_coordinates(.)[,1],
           latitude = st_coordinates(.)[,2]) %>% 
    st_drop_geometry() %>% 
    dplyr::select(-c(X,)) %>% 
    dplyr::select(1,2,3,4,5,260:263,274,275,264:273,256:259,6:255) #Reorder columns to something logical
  
  #Append to relevant year of invert data. i-2001 converts our active year to 1, 2, 3 etc  
  invertlist[[i-2001]] = invertDataSf %>% filter(year == i) %>% 
    vect() %>% 
    terra::extract(y = .,
                   x = stack,
                   bind = TRUE) %>% 
    st_as_sf() %>% 
    st_transform(crs = "EPSG:4326") %>% 
    mutate(longitude = st_coordinates(.)[,1],
           latitude = st_coordinates(.)[,2]) %>% 
    st_drop_geometry() %>% 
    dplyr::select(-c(X,)) %>% 
    dplyr::select(1,2,3,4,5,266:269,280,281,270:279,262:265,6:261) #Reorder columns to something logical
  
  
}

fishData = do.call(rbind, fishlist)
invertData = do.call(rbind, invertlist)

#Now we have a another loop that first check which year and month combination each row was taken from and grabs the associated monthly temp and wave force.
#Fishdata
fishlist = list() #Make new empty holder

fishDataSf = fishData %>% st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% st_transform(crs = "EPSG:7899") #Convert back to SF

yearMonth = fishData$survey_date %>% as.Date(, tz = "Australia/Victoria") %>% format("%Y-%m") %>% unique() %>% sort() #Grab each unique year month combo in the RSL data and order them

for (i in 1:length(yearMonth)){

  print(paste("Working on", yearMonth[i]))
  
  #Grab rasters that correspond to that year month
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Rasters")
  stack = rast(
    list.files() %>% str_subset(pattern = yearMonth[i])) #Get wave and temp rasters. But only the ones that match the unique year-month combo we are iterating on right now.
  
  names(stack) = c("SignifWaveHeightMonthlyMean",
                   "SignifWaveHeightMonthlySD",
                   "TemperatureMonthlyMean")
  
  #Filter all rows where the survey year matches the i entry in our yearMonth object then do the same terra::extractions as we did before
  fishlist[[i]] = 
    fishDataSf[format(as.Date(fishDataSf$survey_date, tz = "Australia/Victoria"), "%Y-%m") == yearMonth[i],] %>% 
    vect() %>% 
    terra::extract(y = .,
                   x = stack,
                   bind = TRUE) %>% 
    st_as_sf() %>% 
    st_transform(crs = "EPSG:4326") %>% 
    mutate(longitude = st_coordinates(.)[,1],
           latitude = st_coordinates(.)[,2]) %>% 
    st_drop_geometry() %>% 
    dplyr::select(1:9,277:278,10:19,20,21,274,275,22,23,276,24:278)
}

fishData = do.call(rbind, fishlist) #Put all the rows back together now they have all the data extracted

#REPEAT FOR INVERTS
invertlist = list() #Make new empty holder

invertDataSf = invertData %>% st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% st_transform(crs = "EPSG:7899") #Convert back to SF

yearMonth = invertData$survey_date %>% as.Date(, tz = "Australia/Victoria") %>% format("%Y-%m") %>% unique() %>% sort() #Grab each unique year month combo in the RSL data and order them

for (i in 1:length(yearMonth)){

  print(paste("Working on", yearMonth[i]))
  
  #Grab rasters that correspond to that year month
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Rasters")
  stack = rast(
    list.files() %>% str_subset(pattern = yearMonth[i])) #Get wave and temp rasters. But only the ones that match the unique year-month combo we are iterating on right now.
  
  names(stack) = c("SignifWaveHeightMonthlyMean",
                   "SignifWaveHeightMonthlySD",
                   "TemperatureMonthlyMean")
  
  #Filter all rows where the survey year matches the i entry in our yearMonth object then do the same terra::extractions as we did before
  invertlist[[i]] = 
    invertDataSf[format(as.Date(invertDataSf$survey_date, tz = "Australia/Victoria"), "%Y-%m") == yearMonth[i],] %>% 
    vect() %>% 
    terra::extract(y = .,
                   x = stack,
                   bind = TRUE) %>% 
    st_as_sf() %>% 
    st_transform(crs = "EPSG:4326") %>% 
    mutate(longitude = st_coordinates(.)[,1],
           latitude = st_coordinates(.)[,2]) %>% 
    st_drop_geometry() %>% 
    dplyr::select(1:9,283:284,10:19,20,21,280,281,22,23,282,24:279)
}

invertData = do.call(rbind, invertlist) #Put all the rows back together now they have all the data extracted

#We also want a new variable time since 2002 (2002 is when the parks became protected).
fishData$YSince2002 = fishData$year - 2002
invertData$YSince2002 = invertData$year - 2002

#Move this new time variable next to the other one.
fishData = fishData %>% 
  relocate(YSince2002, .after = year)

invertData = invertData %>% 
  relocate(YSince2002, .after = year)

#A few surveys were apparently undertaken in >50m of water. Let's drop these. retain anything 30m and above.
fishData = fishData %>% 
  filter(Depth >= -30)

invertData= invertData %>% 
  filter(Depth >= -30)

setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/Modelling/ResponseCSVs")
write.csv(fishData, "fishDataFinal.csv", row.names = FALSE)
write.csv(invertData, "invertDataFinal.csv", row.names = FALSE)




```



### OLD CODE - INLA INTERPALATIONS


Beginning with temperature data as it's the smaller set of data. The inla process can be quite complex....be warned.

Setting up the mesh we need by first importing and setting up the target raster grid and the boundary we'll use. These components are the same across all the variables and timesteps we want to interpolate.

```{r, eval = FALSE}
#Guidance from this link
#https://rpubs.com/jafet089/886687
#Another excellent INLA resource
#https://ourcodingclub.github.io/tutorials/inla/
#Another one
#https://www.paulamoraga.com/book-geospatial/sec-inla.html

#Import the bathymetry layer we are using as our template for xys we want to interpolate onto
templateRast = rast("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/Bathymetry/VICBathy50mMean.tif")

#Make polygon version of the bathymetry
templatePoly = templateRast %>% as.polygons()

#Convert polygon to domain structure INLA wants. Takes a fair while due to size of the polygon we give it.
boundary = templatePoly %>% st_as_sf() %>% as_Spatial() %>% inla.sp2segment()

```

Now we have that done we can work with the data to be interpolated. This will vary between variables and the year being interpolated so it is far more changeable.

Working with temperature data first. The data (temp vs waves) are on different projections at present so we'll split the interpolations into two loops.

```{r, eval = FALSE}
#Calculate the maximum edge length across the data we wish to interpolate (the wave/temp data). Initial guestimate based on the guidance from the links mentioned above.
#Doing this with the first temperature layer as the spatial points are the same between years.

#Remember!!! The temperature data begin in WGS84 as they come from a global dataset.

variableList = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature") %>% 
  str_subset(pattern = "XYZ", negate = TRUE) #Keep only those without. Five temperature measures to work through.

focalYears = readRDS("Z:/GIS-Res/Projects/ParksVic_Hmsc/ResponseData/CoarseLatLon/fishYears") %>% as.character()

for (j in 1:length(variableList)){
  
  setwd("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature")

  filesList = list.files("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature") %>% 
                          str_subset(string = ., pattern = "XYZ", negate = FALSE)
  
  print(str_c("Working on ", filesList[j]))
  
  ToInterpolate = readRDS(filesList[j]) %>% dplyr::select(c(longitude, latitude, focalYears)) %>%  #Read in jth file and drop any columns we dont have animal data for
  st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG7899")
  
for (i in 1:(ncol(ToInterpolate)-1)){ #Skip last col as it's geometry
 
  print(str_c("Interpolating ", names(ToInterpolate)[i], " Data"))
  
  ###################################
  PLACEHOLDER FOR INTERPOLATION CODES
  ##################################
  
   
}
  
}





ToInterpolate = readRDS("Z:/GIS-Res/Projects/ParksVic_Hmsc/EnvironmentalLayers/AggregatedData/Temperature/TemperatureAnnualMeanXYZ")
ToInterpolateXYs = ToInterpolate %>% st_as_sf(coords = c("longitude", "latitude"), crs = "EPSG:4326") %>% st_transform(crs = crs(templateRast)) %>% st_coordinates()
max.edgeL = diff(range(ToInterpolateXYs[,1]))/(3*5)

bound.outer = max.edgeL*5 #Define distance we want our outer boundary to sit outside of the focal extent. Lindgren and Rue (2015) suggest it needs to be at least greater than the range effect. More generally Righettoa et al (2018) found that the cutoff and the max edge length in the inner domain had the greatest impact.
cutoffL = max.edgeL/5 #Distance at which to join adjacent vertices. Stops the mesh having heaps of tiny points crammed in together.



#Extract locations we want to predict onto from the template raster
predictLoc = templateRast %>% as.data.frame(xy = TRUE) %>% as.matrix() #Make points of where we want to compute data for
predictLoc = predictLoc[,1:2] #Convert locations to only xy cols

#Build computational meshes - can take a while due to the small size of the triangles compared to the domain extent.
tic()
mesh <- INLA::inla.mesh.2d(loc=ToInterpolateXYs, boundary = boundary, max.edge = c(1,5)*max.edgeL, offset=c(max.edgeL, bound.outer), cutoff=cutoffL) #Make non-uniform grid across spatial extent of sample locales. Note the boundary argument to stop it training the model in areas we don't want it to
toc()

plot(mesh)

plot(mesh)
plot(ToInterpolate %>% st_as_sf(coords = c("longitude", "latitude"), crs = crs(templatePoly)) %>% as_Spatial(), add=T, col="red")

A.data <- inla.spde.make.A(mesh=mesh, loc=as.matrix(ToInterpolateXYs)) #Make A matrix that is used to map the Gaussian Markov Random Field from the mesh nodes to the n observed locations
spde <- inla.spde2.matern(mesh=mesh, alpha = 2) #Make spatial structure object. Alpha is smoothness parameter, 2 is max smoothness
iset <- inla.spde.make.index(name = "spatial.field", spde$n.spde)

A.data2 <- inla.spde.make.A(mesh=mesh, loc=predictLoc) #Make a matrix used to map the Gaussian Markov Random Field to the locations we want to predict at using the model we will generate using A.data (the first one).

#Stack for model fitting
stk <- inla.stack(data=list(y=masterPPBCropspdf$waterElev), #Response variable 
  A=list(A.data, 1
        ), #A matrix
  effects=list(iset, #Spatial structure object 
  intercept=rep(1,nrow(masterPPBCropspdf))
  ),
  tag="waterElev") #Name for object

#Make a stack for model prediction
stk_pred <- inla.stack(data=list(y=NA),
  A=list(A.data2, 1
  ), #A matrix 
  effects=list(iset, #Spatial structure object 
  intercept=rep(1,nrow(predictLoc))
  ),
  tag="waterElevPred") #Name for object

#Join stacks. The fitting stack and prediction stack need to become one stack to pass to the INLA argument
join.stack <- inla.stack(stk, stk_pred)

#Define simple formula
formula <- y ~ -1 + intercept + f(spatial.field, model=spde) #super simple model. Water elevation predicted is the intercept plus a spatial effect. This is the equivilant of kriging.

tic() #Start timer
mod <- inla(formula, #formula from above
  data=inla.stack.data(join.stack, spde=spde, #data stack
  family="gamma"), #Define distribution
  control.inla = list(h=0.0001),
  control.predictor=list(A = inla.stack.A(join.stack), compute=TRUE, link=1), #Tell INLA what predictor mesh to use, to compute the predictions, and that the predictions need to use the same link as the family specifies. FOr when we vary the family this means that all outputs will automatically be back transformed.
  control.family = list(link = "default"), #Tell INLA to use the default approximation of the gaussian family
  control.compute=list(openmp.strategy="huge", cpo=TRUE, dic=TRUE,
  return.marginals=TRUE),#INLA uses different degrees of parallelism depending on how big of a job it sees the model as. Let's see if setting it to huge speeds up the process.
  num.threads = detectCores()-1) #Tell INLA it can use all but 1 core for computation.
toc() #End timers

tic()
hyper <- inla.hyperpar(mod) #Put outputs of hyperparameters into a separate object. This function re-calculates the posterior estimates of the hyperparams and apparently makes better estimates than those in the mod object above. R documentation says it may take a long time.
toc()

```

